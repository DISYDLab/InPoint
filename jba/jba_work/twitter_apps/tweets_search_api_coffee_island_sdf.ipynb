{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tweets\n",
    "\n",
    "This script extracts all the tweets with hashtag #covid-19 related to the day before today (yesterday) and saves them into a .csv file.\n",
    "We use the `tweepy` library, which can be installed with the command `pip install tweepy`.\n",
    "\n",
    "Firstly, we import the configuration file, called `config.py`, which is located in the same directory of this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/opt/spark/python',\n",
       " '/opt/spark/python/lib/py4j-0.10.9-src.zip',\n",
       " '/home/hadoopuser/Documents/twitter_search_api',\n",
       " '/home/hadoopuser/.vscode/extensions/ms-toolsai.jupyter-2021.6.999662501/pythonFiles/vscode_datascience_helpers',\n",
       " '/home/hadoopuser/.vscode/extensions/ms-toolsai.jupyter-2021.6.999662501/pythonFiles',\n",
       " '/home/hadoopuser/.vscode/extensions/ms-toolsai.jupyter-2021.6.999662501/pythonFiles/lib/python',\n",
       " '/opt/anaconda/envs/pyspark_env/lib/python37.zip',\n",
       " '/opt/anaconda/envs/pyspark_env/lib/python3.7',\n",
       " '/opt/anaconda/envs/pyspark_env/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/opt/anaconda/envs/pyspark_env/lib/python3.7/site-packages',\n",
       " '/opt/anaconda/envs/pyspark_env/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/home/hadoopuser/.ipython']"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import os\n",
    "jv = os.environ.get('JAVA_HOME', None)\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "'--packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.0 pyspark-shell'\n",
    "# '--packages org.postgresql:postgresql:42.1.1 pyspark-shell'\n",
    "\n",
    "sys.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, glob, os\n",
    "sys.path.extend(glob.glob(os.path.join(os.path.expanduser(\"~\"), \".ivy2/jars/*.jar\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark=SparkSession.builder \\\n",
    ".appName(\"Spark_and_Pandas_twitter_dfs\") \\\n",
    ".master(\"local[*]\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdee3fbe450>"
      ],
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://spark-client:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Spark_and_Pandas_twitter_dfs</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")"
   ]
  },
  {
   "source": [
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`result_pdf.head(2)`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mypy\n",
    "from config import *\n",
    "import tweepy\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('tweets_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "logger.root.level = 30, logger.root.name = root\nlogger.name = tweets_search\n"
     ]
    }
   ],
   "source": [
    "print(f\"logger.root.level = {logger.root.level}, logger.root.name = {logger.root.name}\")\n",
    "print(f\"logger.name = {logger.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "# logging.basicConfig(format=format, stream=sys.stdout, level = logging.DEBUG)\n",
    "logging.basicConfig(format=format, stream=sys.stdout, level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(logger.root.level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.root.level = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# print(logger.root.level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup the connection to our Twitter App by using the `OAuthHandler()` class and its `access_token()` function. Then we call the Twitter API through the `API()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(TWITTER_CONSUMER_KEY, TWITTER_CONSUMER_SECRET)\n",
    "auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True, wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api.me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api.rate_limit_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup dates (recent 7 days max)\n",
    "```\n",
    "pdf['created_at'].dt.strftime('%Y-%m-%d')\n",
    "- provide since and until dates (date_format = 'Y-m-d', i.e '2021-01-30')\n",
    "since:'', until:'', or\n",
    "- provide timedelta (until= datetime.date.today(), since= datetime.date.today()-timedelta), i.e timedelta: '2', or\n",
    "- setup\n",
    "\n",
    "\n",
    "Now we setup dates. We need to setup today and yesterday.\n",
    "```"
   ]
  },
  {
   "source": [
    "## setup dates (recent 7 days max)\n",
    "\n",
    "If today is 2021-06-26 then :\n",
    "\n",
    "1. `time_frame = {timedelta:'2'}` (we get tweets from 2021-0-24 up to 2021-06-25 (today - 1 day))\n",
    "2. `time_frame = {since:'2021-06-23', timedelta:'2'}` \n",
    "3. `time_frame = {until:'2021-06-25', timedelta:'2'}` (2 & 3 & 4 expressions are equivalent)\n",
    "4. `time_frame = {since:'2021-06-23', until:'2021-06-25'}` -> we get tweets from 2021-06-23 up to 2021-06-24\n",
    "\n",
    "`note:` from today we can get a time_frame of 7 days max, i.e since 2021-06-19"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "datetime.date(2021, 1, 30)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# https://www.educative.io/edpresso/how-to-convert-a-string-to-a-date-in-python\n",
    "import datetime\n",
    "since = '2021-01-30'\n",
    "datetime.datetime.strptime(since, '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(datetime.date(2021, 6, 28), datetime.date(2021, 6, 26))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "today = datetime.date.today()\n",
    "since= today - datetime.timedelta(days=2)\n",
    "until= today\n",
    "until, since\n",
    "# (datetime.date(2021, 6, 7), datetime.date(2021, 6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(f\"time_frame: '{until, since}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We search for tweets on Twitter by using the `Cursor()` function. \n",
    "We pass the `api.search` parameter to the cursor, as well as the query string, which is specified through the `q` parameter of the cursor.\n",
    "The query string can receive many parameters, such as the following (not mandatory) ones:\n",
    "* `from:` - to specify a specific Twitter user profile\n",
    "* `since:` - to specify the beginning date of search\n",
    "* `until:` - to specify the ending date of search\n",
    "The cursor can also receive other parameters, such as the language and the `tweet_mode`. If `tweet_mode='extended'`, all the text of the tweet is returned, otherwise only the first 140 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example \n",
    "# code tweets = tweepy.Cursor(api.search, tweet_mode=’extended’) \n",
    "# for tweet in tweets:\n",
    "#     content = tweet.full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_list = tweepy.Cursor(api.search, q=\"#Covid-19 since:\" + str(yesterday)+ \" until:\" + str(today),tweet_mode='extended', lang='en').items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_list = tweepy.Cursor(api.search, q=f\"#Covid-19 since:{str(yesterday)} until:{str(today)}\",tweet_mode='extended', lang='en').items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_list = tweepy.Cursor(api.search, q=['astrazeneca', 'pfizer'],since= str(since), until=str(until),tweet_mode='extended', lang='en').items()\n",
    "\n",
    "# Greek Language = el\n",
    "# tweets_list = tweepy.Cursor(api.search, q=['coffee island'],since= str(since), until=str(until),tweet_mode='extended', lang='el').items()\n",
    "\n",
    "# English Language = en\n",
    "tweets_list = tweepy.Cursor(api.search, q=['coffee island OR CoffeeIsland'],since= str(since), until=str(until),tweet_mode='extended', lang='en').items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop across the `tweets_list`, and, for each tweet, we extract the text, the creation date, the number of retweets and the favourite count. We store every tweet into a list, called `output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-28 21:15:19,468 - INFO - elapsed_time: '5.003556251525879'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "seconds = 5\n",
    "start = time.time()\n",
    "time.sleep(seconds)\n",
    "end = time.time()\n",
    "logger.info(f\"elapsed_time: '{end - start}'\")"
   ]
  },
  {
   "source": [
    "---\n",
    "# TEST"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t™️\n",
      "2021-06-28 21:16:22,730 - INFO - user_id: 1194346351179501576\n",
      "2021-06-28 21:16:22,733 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:22,734 - INFO - created_at: 2021-06-26 14:27:32\n",
      "2021-06-28 21:16:22,735 - INFO - full_text: ***Very Strawberry &amp; Blue Cotton Candy***\n",
      "Chocolate Ice Cream &amp; Vanilla Ice Cream\n",
      "NSA Vanilla &amp; Campfire Crush\n",
      "New York Cheesecake &amp; Pineapple Dole Soft Serve\n",
      "Cold Brew Coffee Gelato &amp; Cake Batter\n",
      "Sweet Coconut &amp; Island Banana\n",
      "Cookie n' Cream &amp; Caramel Sea Salt Gelato https://t.co/qT474mhoyy\n",
      "2021-06-28 21:16:22,735 - INFO - tweet_id: 1408794058647035905\n",
      "2021-06-28 21:16:22,736 - INFO - tweet_id_str: 1408794058647035905\n",
      "2021-06-28 21:16:22,737 - INFO - user: sweetFrog Dundalk\n",
      "2021-06-28 21:16:22,749 - INFO - user_id: 1895585546\n",
      "2021-06-28 21:16:22,750 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:22,751 - INFO - created_at: 2021-06-26 14:07:13\n",
      "2021-06-28 21:16:22,755 - INFO - full_text: LIVE!\n",
      "\n",
      "Good morning. Just made some coffee, stirred in a little creamer with this tiny cat spoon then proceeded to get yelled at by my cat.\n",
      "\n",
      "Working more on our new island, see you in there!\n",
      "\n",
      "https://t.co/zkERJII1iE https://t.co/Qb8lUYgm6W\n",
      "2021-06-28 21:16:22,757 - INFO - tweet_id: 1408788945572405259\n",
      "2021-06-28 21:16:22,758 - INFO - tweet_id_str: 1408788945572405259\n",
      "2021-06-28 21:16:22,758 - INFO - user: CrossingCasey\n",
      "2021-06-28 21:16:22,759 - INFO - user_id: 148443303\n",
      "2021-06-28 21:16:22,759 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:22,760 - INFO - created_at: 2021-06-26 13:56:29\n",
      "2021-06-28 21:16:22,761 - INFO - full_text: Funny how a song can transport you. Vanessa Carlton’s on the radio and I’m 20 again just starting my J1 visa working in the National hotel on Block Island, USA, currently on coffee break in a cafe https://t.co/s9SyKQucda\n",
      "2021-06-28 21:16:22,761 - INFO - tweet_id: 1408786247083040768\n",
      "2021-06-28 21:16:22,764 - INFO - tweet_id_str: 1408786247083040768\n",
      "2021-06-28 21:16:22,765 - INFO - user: DrCath\n",
      "2021-06-28 21:16:22,767 - INFO - user_id: 57687124\n",
      "2021-06-28 21:16:22,772 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:22,773 - INFO - created_at: 2021-06-26 13:34:08\n",
      "2021-06-28 21:16:22,779 - INFO - full_text: RT @cathsherman: #Bahamas Fishing Pier, Great Abaco Island.\n",
      "#GreatAbaco #FineArtAmerica #CLS\n",
      "#ArtForSale \n",
      "All Products: https://t.co/2BPDbE…\n",
      "2021-06-28 21:16:22,780 - INFO - tweet_id: 1408780619409149956\n",
      "2021-06-28 21:16:22,781 - INFO - tweet_id_str: 1408780619409149956\n",
      "2021-06-28 21:16:22,781 - INFO - user: Judy Vincent\n",
      "2021-06-28 21:16:22,782 - INFO - user_id: 2543593783\n",
      "2021-06-28 21:16:22,782 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:22,783 - INFO - created_at: 2021-06-26 13:30:22\n",
      "2021-06-28 21:16:22,783 - INFO - full_text: Since we can’t travel to Canary Island we brought Canaries to Evuna..\n",
      "\n",
      "Barraquito Coffee..\n",
      "\n",
      "Layers of Liquour 43, condensed milk, coffee and milk finished off with cinnamon and lemon zest. https://t.co/PFFUb59TAq\n",
      "2021-06-28 21:16:22,784 - INFO - tweet_id: 1408779672163340291\n",
      "2021-06-28 21:16:22,784 - INFO - tweet_id_str: 1408779672163340291\n",
      "2021-06-28 21:16:22,785 - INFO - user: Evuna\n",
      "2021-06-28 21:16:22,786 - INFO - user_id: 54543636\n",
      "2021-06-28 21:16:22,787 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:22,789 - INFO - created_at: 2021-06-26 13:25:26\n",
      "2021-06-28 21:16:22,790 - INFO - full_text: Read Treasure Island and drink some chilled water from a irish coffee glass.\n",
      "2021-06-28 21:16:22,799 - INFO - tweet_id: 1408778431878344711\n",
      "2021-06-28 21:16:22,800 - INFO - tweet_id_str: 1408778431878344711\n",
      "2021-06-28 21:16:22,800 - INFO - user: abookandbeverage\n",
      "2021-06-28 21:16:22,801 - INFO - user_id: 1368800100705734659\n",
      "2021-06-28 21:16:22,802 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:22,802 - INFO - created_at: 2021-06-26 13:09:02\n",
      "2021-06-28 21:16:22,803 - INFO - full_text: Picked this up in a little coffee shop on a tropical island in the pacific. Memorable vacation, that one was. #JurassicWorld #JurassicPark #JurassicWorldDominion https://t.co/0bxBJE1CNU\n",
      "2021-06-28 21:16:22,804 - INFO - tweet_id: 1408774303970217994\n",
      "2021-06-28 21:16:22,840 - INFO - tweet_id_str: 1408774303970217994\n",
      "2021-06-28 21:16:22,846 - INFO - user: The Cretaceous Dad\n",
      "2021-06-28 21:16:22,854 - INFO - user_id: 1396850083556208642\n",
      "2021-06-28 21:16:22,858 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:22,861 - INFO - created_at: 2021-06-26 12:00:36\n",
      "2021-06-28 21:16:22,865 - INFO - full_text: Coney Island restaurant for breakfast &amp; coffee\n",
      "2021-06-28 21:16:22,867 - INFO - tweet_id: 1408757081847566338\n",
      "2021-06-28 21:16:22,873 - INFO - tweet_id_str: 1408757081847566338\n",
      "2021-06-28 21:16:22,875 - INFO - user: 🦉\n",
      "2021-06-28 21:16:22,877 - INFO - user_id: 900689740743274496\n",
      "2021-06-28 21:16:22,902 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:22,906 - INFO - created_at: 2021-06-26 11:05:23\n",
      "2021-06-28 21:16:22,909 - INFO - full_text: and jennie got tired of moving from one country to another just to keep a safe distance away from the problematic family business and went to jeju island where she stayed at a famous coffee shop owned by jisoo’s grandfather\n",
      "\n",
      "wow. i’m totally bored\n",
      "2021-06-28 21:16:22,911 - INFO - tweet_id: 1408743188173570048\n",
      "2021-06-28 21:16:22,920 - INFO - tweet_id_str: 1408743188173570048\n",
      "2021-06-28 21:16:22,922 - INFO - user: ang\n",
      "2021-06-28 21:16:22,929 - INFO - user_id: 229469475\n",
      "2021-06-28 21:16:22,932 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:22,936 - INFO - created_at: 2021-06-26 11:04:16\n",
      "2021-06-28 21:16:22,939 - INFO - full_text: RT @mir_gifs: June 19th bunny and animal crossing island coffee status ☕️🐰🐶 https://t.co/cHdGkmEMN1\n",
      "2021-06-28 21:16:22,945 - INFO - tweet_id: 1408742904147824641\n",
      "2021-06-28 21:16:22,948 - INFO - tweet_id_str: 1408742904147824641\n",
      "2021-06-28 21:16:22,949 - INFO - user: an alright potato peeler.\n",
      "2021-06-28 21:16:22,955 - INFO - user_id: 750698074058551297\n",
      "2021-06-28 21:16:22,956 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:22,959 - INFO - created_at: 2021-06-26 11:00:03\n",
      "2021-06-28 21:16:22,961 - INFO - full_text: Good Morning, I've got the coffee on for you.\n",
      "2021-06-28 21:16:22,964 - INFO - tweet_id: 1408741845102891011\n",
      "2021-06-28 21:16:22,980 - INFO - tweet_id_str: 1408741845102891011\n",
      "2021-06-28 21:16:22,984 - INFO - user: jojo writer\n",
      "2021-06-28 21:16:22,986 - INFO - user_id: 155903774\n",
      "2021-06-28 21:16:22,990 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:22,995 - INFO - created_at: 2021-06-26 10:13:24\n",
      "2021-06-28 21:16:22,997 - INFO - full_text: i wanna move to staten island and get a cat and open a coffee shop 👍\n",
      "2021-06-28 21:16:23,003 - INFO - tweet_id: 1408730105086685189\n",
      "2021-06-28 21:16:23,005 - INFO - tweet_id_str: 1408730105086685189\n",
      "2021-06-28 21:16:23,009 - INFO - user: ꪻꪖꪑꪖ ✿\n",
      "2021-06-28 21:16:23,011 - INFO - user_id: 1339077661751201794\n",
      "2021-06-28 21:16:23,012 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:23,014 - INFO - created_at: 2021-06-26 10:12:53\n",
      "2021-06-28 21:16:23,021 - INFO - full_text: On the 2nd anniversary of the day I planned to die in going to Majorca. I need these things to look forward to. Little things like coffee with a friend, a curry night... Bigger things, like getting on a plan and leaving this shit little island for 10 days.\n",
      "2021-06-28 21:16:23,027 - INFO - tweet_id: 1408729975432388612\n",
      "2021-06-28 21:16:23,033 - INFO - tweet_id_str: 1408729975432388612\n",
      "2021-06-28 21:16:23,034 - INFO - user: Lady Amelia ☕ 🖖🦎\n",
      "2021-06-28 21:16:23,035 - INFO - user_id: 787344251319353344\n",
      "2021-06-28 21:16:23,040 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:23,041 - INFO - created_at: 2021-06-26 10:06:41\n",
      "2021-06-28 21:16:23,046 - INFO - full_text: @need_coffee_now Proper job, Stuart. This sort of jingoism that musn't take root on this island.\n",
      "2021-06-28 21:16:23,047 - INFO - tweet_id: 1408728415943995393\n",
      "2021-06-28 21:16:23,050 - INFO - tweet_id_str: 1408728415943995393\n",
      "2021-06-28 21:16:23,057 - INFO - user: YES Kernow〓〓🇮🇪🏴󠁧󠁢󠁳󠁣󠁴󠁿🏴󠁧󠁢󠁷󠁬󠁳󠁿\n",
      "2021-06-28 21:16:23,062 - INFO - user_id: 3070119119\n",
      "2021-06-28 21:16:24,084 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,085 - INFO - created_at: 2021-06-26 09:37:03\n",
      "2021-06-28 21:16:24,086 - INFO - full_text: @UnknowingTitan [Groans into the floor.] Hanji gave me this weird coffee. We ran a few miles, I did a bunch of cartwheels in my titan, and then I think I fell off the wall after I climbed halfway up. Hanji cut my arm off getting me out. I feel like every horse on the island trampled me, twice.\n",
      "2021-06-28 21:16:24,086 - INFO - tweet_id: 1408720957385199617\n",
      "2021-06-28 21:16:24,087 - INFO - tweet_id_str: 1408720957385199617\n",
      "2021-06-28 21:16:24,088 - INFO - user: Eren ☼\n",
      "2021-06-28 21:16:24,089 - INFO - user_id: 1371048516387278849\n",
      "2021-06-28 21:16:24,096 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,096 - INFO - created_at: 2021-06-26 09:26:44\n",
      "2021-06-28 21:16:24,105 - INFO - full_text: A nice latte in Rhode Island coffee! https://t.co/6vRDOsXsK5\n",
      "2021-06-28 21:16:24,108 - INFO - tweet_id: 1408718358611214338\n",
      "2021-06-28 21:16:24,109 - INFO - tweet_id_str: 1408718358611214338\n",
      "2021-06-28 21:16:24,109 - INFO - user: Jeff\n",
      "2021-06-28 21:16:24,110 - INFO - user_id: 379266340\n",
      "2021-06-28 21:16:24,111 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,111 - INFO - created_at: 2021-06-26 09:17:01\n",
      "2021-06-28 21:16:24,112 - INFO - full_text: @TAHK0 Still mad at island limitations per console, ngl. My partner still loves it tho so cant be too mad.\n",
      "2021-06-28 21:16:24,112 - INFO - tweet_id: 1408715914560823297\n",
      "2021-06-28 21:16:24,113 - INFO - tweet_id_str: 1408715914560823297\n",
      "2021-06-28 21:16:24,113 - INFO - user: Boyo\n",
      "2021-06-28 21:16:24,114 - INFO - user_id: 907036710646816769\n",
      "2021-06-28 21:16:24,115 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,115 - INFO - created_at: 2021-06-26 08:30:42\n",
      "2021-06-28 21:16:24,116 - INFO - full_text: 🥥 Imagine a Bounty dunked in coffee... Subtle hints of coconut adds a tropical twist to our smooth Arabica coffee and will have you on island time in no time 🏝 \n",
      "\n",
      "#nationalcoconutday #livenupyourcup #coconutcoffee #flavouredcoffee https://t.co/abvWkRtjLk\n",
      "2021-06-28 21:16:24,117 - INFO - tweet_id: 1408704259504459778\n",
      "2021-06-28 21:16:24,122 - INFO - tweet_id_str: 1408704259504459778\n",
      "2021-06-28 21:16:24,123 - INFO - user: We Are Little's\n",
      "2021-06-28 21:16:24,132 - INFO - user_id: 309619814\n",
      "2021-06-28 21:16:24,133 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,134 - INFO - created_at: 2021-06-26 07:51:26\n",
      "2021-06-28 21:16:24,134 - INFO - full_text: In the bakers in town. Best coffee on the island \n",
      "Steely Dan on in background \n",
      "What a time to be alive https://t.co/U5UZ5J8lVf\n",
      "2021-06-28 21:16:24,135 - INFO - tweet_id: 1408694378118406147\n",
      "2021-06-28 21:16:24,136 - INFO - tweet_id_str: 1408694378118406147\n",
      "2021-06-28 21:16:24,137 - INFO - user: Lorenzo 🇬🇧🏴󠁧󠁢󠁷󠁬󠁳󠁿🇪🇸\n",
      "2021-06-28 21:16:24,137 - INFO - user_id: 1249332378704764928\n",
      "2021-06-28 21:16:24,138 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,140 - INFO - created_at: 2021-06-26 06:20:51\n",
      "2021-06-28 21:16:24,144 - INFO - full_text: RT @VectisRadio: Join Maggie for the Breakfast Show, sponsored by Island Environmental Hygiene Ltd, from 8am.  Hear great music, local musi…\n",
      "2021-06-28 21:16:24,147 - INFO - tweet_id: 1408671580394295298\n",
      "2021-06-28 21:16:24,153 - INFO - tweet_id_str: 1408671580394295298\n",
      "2021-06-28 21:16:24,154 - INFO - user: The 4P's Vectis Radio Training School\n",
      "2021-06-28 21:16:24,154 - INFO - user_id: 1393130350461788162\n",
      "2021-06-28 21:16:24,156 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,158 - INFO - created_at: 2021-06-26 06:20:46\n",
      "2021-06-28 21:16:24,159 - INFO - full_text: RT @VectisRadio: Join Maggie for the Breakfast Show, sponsored by Island Environmental Hygiene Ltd, from 8am.  Hear great music, local musi…\n",
      "2021-06-28 21:16:24,161 - INFO - tweet_id: 1408671559489818624\n",
      "2021-06-28 21:16:24,161 - INFO - tweet_id_str: 1408671559489818624\n",
      "2021-06-28 21:16:24,164 - INFO - user: Maggie Currie\n",
      "2021-06-28 21:16:24,165 - INFO - user_id: 14510070\n",
      "2021-06-28 21:16:24,166 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,167 - INFO - created_at: 2021-06-26 06:20:37\n",
      "2021-06-28 21:16:24,168 - INFO - full_text: Join Maggie for the Breakfast Show, sponsored by Island Environmental Hygiene Ltd, from 8am.  Hear great music, local music from Glenn Koppany and Bethan John Music, a motivational moment, how to join our virtual coffee morning and much more.  See you there, https://t.co/T09RR5pbrW\n",
      "2021-06-28 21:16:24,171 - INFO - tweet_id: 1408671520914915328\n",
      "2021-06-28 21:16:24,177 - INFO - tweet_id_str: 1408671520914915328\n",
      "2021-06-28 21:16:24,178 - INFO - user: Vectis Radio\n",
      "2021-06-28 21:16:24,179 - INFO - user_id: 108095592\n",
      "2021-06-28 21:16:24,180 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,180 - INFO - created_at: 2021-06-26 05:54:13\n",
      "2021-06-28 21:16:24,181 - INFO - full_text: today I heard a woman at fashion island refer to Arabic coffee as a “coffee shot” and I want to die\n",
      "2021-06-28 21:16:24,182 - INFO - tweet_id: 1408664879720910849\n",
      "2021-06-28 21:16:24,186 - INFO - tweet_id_str: 1408664879720910849\n",
      "2021-06-28 21:16:24,186 - INFO - user: Diana Jarrah\n",
      "2021-06-28 21:16:24,187 - INFO - user_id: 4006048213\n",
      "2021-06-28 21:16:24,188 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,190 - INFO - created_at: 2021-06-26 04:50:16\n",
      "2021-06-28 21:16:24,194 - INFO - full_text: RT @cathsherman: #Bahamas Fishing Pier, Great Abaco Island.\n",
      "#GreatAbaco #FineArtAmerica #CLS\n",
      "#ArtForSale \n",
      "All Products: https://t.co/2BPDbE…\n",
      "2021-06-28 21:16:24,196 - INFO - tweet_id: 1408648785648046085\n",
      "2021-06-28 21:16:24,196 - INFO - tweet_id_str: 1408648785648046085\n",
      "2021-06-28 21:16:24,197 - INFO - user: Andrea Anderegg\n",
      "2021-06-28 21:16:24,198 - INFO - user_id: 80315907\n",
      "2021-06-28 21:16:24,200 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,211 - INFO - created_at: 2021-06-26 04:49:30\n",
      "2021-06-28 21:16:24,212 - INFO - full_text: Lately been obsessed over aquariums in unorthodox places like kitchen island, a bar top, coffee tables.\n",
      "\n",
      "I want aquariums in every room in the house\n",
      "2021-06-28 21:16:24,213 - INFO - tweet_id: 1408648594446553088\n",
      "2021-06-28 21:16:24,214 - INFO - tweet_id_str: 1408648594446553088\n",
      "2021-06-28 21:16:24,216 - INFO - user: Meechaveli\n",
      "2021-06-28 21:16:24,216 - INFO - user_id: 576791857\n",
      "2021-06-28 21:16:24,218 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,220 - INFO - created_at: 2021-06-26 04:35:16\n",
      "2021-06-28 21:16:24,222 - INFO - full_text: RT @UWCNVI: The #Nanaimo 7-10 Club is running a cooling centre at 285 Prideaux Jun 26-Jul 10 supported by @UWCNVI, @cityofnanaimo + @CMHA_M…\n",
      "2021-06-28 21:16:24,224 - INFO - tweet_id: 1408645010841489408\n",
      "2021-06-28 21:16:24,224 - INFO - tweet_id_str: 1408645010841489408\n",
      "2021-06-28 21:16:24,228 - INFO - user: danny\n",
      "2021-06-28 21:16:24,229 - INFO - user_id: 1371237175\n",
      "2021-06-28 21:16:24,230 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,230 - INFO - created_at: 2021-06-26 03:45:05\n",
      "2021-06-28 21:16:24,236 - INFO - full_text: RT @visit_kochi: Kashiwa is located at the southwestern tip of Kochi, and boasts waters so clear that boats look like they are suspended in…\n",
      "2021-06-28 21:16:24,237 - INFO - tweet_id: 1408632381829828611\n",
      "2021-06-28 21:16:24,238 - INFO - tweet_id_str: 1408632381829828611\n",
      "2021-06-28 21:16:24,239 - INFO - user: チェルベロコーヒー\n",
      "2021-06-28 21:16:24,240 - INFO - user_id: 1150803547\n",
      "2021-06-28 21:16:24,241 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,242 - INFO - created_at: 2021-06-26 03:34:28\n",
      "2021-06-28 21:16:24,249 - INFO - full_text: @jeremarketer You won’t regret it, if you can score some 100% Kona coffee it’s like no other. Order direct from the Big Island of Hawaii\n",
      "2021-06-28 21:16:24,250 - INFO - tweet_id: 1408629709546561540\n",
      "2021-06-28 21:16:24,251 - INFO - tweet_id_str: 1408629709546561540\n",
      "2021-06-28 21:16:24,252 - INFO - user: Catwithsixlivesleft\n",
      "2021-06-28 21:16:24,253 - INFO - user_id: 1009497903839080448\n",
      "2021-06-28 21:16:24,255 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:24,257 - INFO - created_at: 2021-06-26 03:22:54\n",
      "2021-06-28 21:16:24,261 - INFO - full_text: RT @CatherineSher10: A statue of Hawaiian King Kamehameha the Great, Wailoa State Park in #Hilo #Hawaii on the Big Island. 🌺\n",
      "#BigIsland #CL…\n",
      "2021-06-28 21:16:24,262 - INFO - tweet_id: 1408626799999741953\n",
      "2021-06-28 21:16:24,268 - INFO - tweet_id_str: 1408626799999741953\n",
      "2021-06-28 21:16:24,271 - INFO - user: Catherine\n",
      "2021-06-28 21:16:24,277 - INFO - user_id: 233213435\n",
      "2021-06-28 21:16:25,061 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:25,071 - INFO - created_at: 2021-06-26 03:20:48\n",
      "2021-06-28 21:16:25,075 - INFO - full_text: RT @CatherineSher10: A statue of Hawaiian King Kamehameha the Great, Wailoa State Park in #Hilo #Hawaii on the Big Island. 🌺\n",
      "#BigIsland #CL…\n",
      "2021-06-28 21:16:25,077 - INFO - tweet_id: 1408626271089664000\n",
      "2021-06-28 21:16:25,079 - INFO - tweet_id_str: 1408626271089664000\n",
      "2021-06-28 21:16:25,085 - INFO - user: Carol Groenen\n",
      "2021-06-28 21:16:25,086 - INFO - user_id: 36961559\n",
      "2021-06-28 21:16:25,087 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:25,088 - INFO - created_at: 2021-06-26 03:18:36\n",
      "2021-06-28 21:16:25,091 - INFO - full_text: RT @cathsherman: #Bahamas Fishing Pier, Great Abaco Island.\n",
      "#GreatAbaco #FineArtAmerica #CLS\n",
      "#ArtForSale \n",
      "All Products: https://t.co/2BPDbE…\n",
      "2021-06-28 21:16:25,092 - INFO - tweet_id: 1408625715407327232\n",
      "2021-06-28 21:16:25,093 - INFO - tweet_id_str: 1408625715407327232\n",
      "2021-06-28 21:16:25,093 - INFO - user: Carol Groenen\n",
      "2021-06-28 21:16:25,094 - INFO - user_id: 36961559\n",
      "2021-06-28 21:16:25,099 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:25,100 - INFO - created_at: 2021-06-26 01:53:43\n",
      "2021-06-28 21:16:25,100 - INFO - full_text: @UnclePlasma Yes that's right. And I found it out to be both actually. The day I figured that out we had a game at Battin Island, which happened to be in coffee weather. When I sat down to watch the game with a cup of coffee, thats when he reached out to me, as a face in the cup.\n",
      "2021-06-28 21:16:25,101 - INFO - tweet_id: 1408604354819637255\n",
      "2021-06-28 21:16:25,101 - INFO - tweet_id_str: 1408604354819637255\n",
      "2021-06-28 21:16:25,102 - INFO - user: Mindy Salad 💋🥗🔍\n",
      "2021-06-28 21:16:25,102 - INFO - user_id: 1396533553190608898\n",
      "2021-06-28 21:16:25,102 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:25,103 - INFO - created_at: 2021-06-26 00:50:02\n",
      "2021-06-28 21:16:25,103 - INFO - full_text: Good Morning\n",
      "\n",
      "Good Coffee makes the day happier @ Golf Island PIK https://t.co/ydDc4ulXqk\n",
      "2021-06-28 21:16:25,104 - INFO - tweet_id: 1408588328048508936\n",
      "2021-06-28 21:16:25,104 - INFO - tweet_id_str: 1408588328048508936\n",
      "2021-06-28 21:16:25,105 - INFO - user: Benny Sofjan\n",
      "2021-06-28 21:16:25,105 - INFO - user_id: 4080316334\n",
      "2021-06-28 21:16:25,107 - INFO - tweet_id_str: ------------------------------\n",
      "2021-06-28 21:16:25,107 - INFO - created_at: 2021-06-26 00:42:05\n",
      "2021-06-28 21:16:25,108 - INFO - full_text: RT @SafemoonWarrior: #SAFEMOON🕵🏻‍♂️ \n",
      "Can someone tell if this is a country / island / territory? Probably coffee producer. \n",
      "\n",
      "Karony said so…\n",
      "2021-06-28 21:16:25,114 - INFO - tweet_id: 1408586328455843844\n",
      "2021-06-28 21:16:25,115 - INFO - tweet_id_str: 1408586328455843844\n",
      "2021-06-28 21:16:25,116 - INFO - user: Cissie-Lou Rodriguez\n",
      "2021-06-28 21:16:25,116 - INFO - user_id: 1390833500261998593\n"
     ]
    }
   ],
   "source": [
    "# tweets_list2 = tweepy.Cursor(api.search, q=['pfizer','astrazeneca'],since= str(since), until=str(until),tweet_mode='extended', lang='en').items(2)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "output = []\n",
    "for tweet in tweets_list:\n",
    "    # text = tweet._json[\"full_text\"]\n",
    "    #print(text) \n",
    "    # https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets           \n",
    "    # \"geo\": null,\"coordinates\": null,\"place\": null,\"contributors\": null,\n",
    "    # \"is_quote_status\": false,\"retweet_count\": 988,\"favorite_count\": 3875,\n",
    "    # \"favorited\": false,\"retweeted\": false,\"possibly_sensitive\": false,\"lang\": \"en\"\n",
    "    # https://developer.twitter.com/en/docs/twitter-ids\n",
    "    logger.info(f\"tweet_id_str: {'-'*30}\")\n",
    "    logger.info(f\"created_at: {tweet.created_at}\")\n",
    "    logger.info(f\"full_text: {tweet._json['full_text']}\")\n",
    "    logger.info(f\"tweet_id: {tweet.id}\")\n",
    "    logger.info(f\"tweet_id_str: {tweet.id_str}\")\n",
    "    logger.info(f\"user: {tweet._json['user']['name']}\")\n",
    "    logger.info(f\"user_id: {tweet._json['user']['id']}\")    \n",
    "    # favourite_count = tweet.favorite_count\n",
    "    # retweet_count = tweet.retweet_count\n",
    "    # created_at = tweet.created_at\n",
    "    \n",
    "#     line = {'text' : text, 'favourite_count' : favourite_count, 'retweet_count' : retweet_count, 'created_at' : created_at}\n",
    "#     output.append(line)\n",
    "#     logger.info(f\"Append list length : { len(output)}\")\n",
    "# end = time.time()\n",
    "# logger.info(f\"elapsed_time: '{end - start}'\")"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-28 21:19:47,664 - INFO - Append list length : 1\n",
      "2021-06-28 21:19:47,670 - INFO - Append list length : 2\n",
      "2021-06-28 21:19:47,675 - INFO - Append list length : 3\n",
      "2021-06-28 21:19:47,676 - INFO - Append list length : 4\n",
      "2021-06-28 21:19:47,680 - INFO - Append list length : 5\n",
      "2021-06-28 21:19:47,682 - INFO - Append list length : 6\n",
      "2021-06-28 21:19:47,683 - INFO - Append list length : 7\n",
      "2021-06-28 21:19:47,684 - INFO - Append list length : 8\n",
      "2021-06-28 21:19:47,686 - INFO - Append list length : 9\n",
      "2021-06-28 21:19:47,686 - INFO - Append list length : 10\n",
      "2021-06-28 21:19:47,690 - INFO - Append list length : 11\n",
      "2021-06-28 21:19:47,696 - INFO - Append list length : 12\n",
      "2021-06-28 21:19:47,699 - INFO - Append list length : 13\n",
      "2021-06-28 21:19:47,704 - INFO - Append list length : 14\n",
      "2021-06-28 21:19:47,705 - INFO - Append list length : 15\n",
      "2021-06-28 21:19:54,601 - INFO - Append list length : 16\n",
      "2021-06-28 21:19:54,602 - INFO - Append list length : 17\n",
      "2021-06-28 21:19:54,609 - INFO - Append list length : 18\n",
      "2021-06-28 21:19:54,615 - INFO - Append list length : 19\n",
      "2021-06-28 21:19:54,620 - INFO - Append list length : 20\n",
      "2021-06-28 21:19:54,624 - INFO - Append list length : 21\n",
      "2021-06-28 21:19:54,625 - INFO - Append list length : 22\n",
      "2021-06-28 21:19:54,626 - INFO - Append list length : 23\n",
      "2021-06-28 21:19:54,626 - INFO - Append list length : 24\n",
      "2021-06-28 21:19:54,632 - INFO - Append list length : 25\n",
      "2021-06-28 21:19:54,633 - INFO - Append list length : 26\n",
      "2021-06-28 21:19:54,634 - INFO - Append list length : 27\n",
      "2021-06-28 21:19:54,635 - INFO - Append list length : 28\n",
      "2021-06-28 21:19:54,637 - INFO - Append list length : 29\n",
      "2021-06-28 21:19:54,640 - INFO - Append list length : 30\n",
      "2021-06-28 21:19:56,549 - INFO - Append list length : 31\n",
      "2021-06-28 21:19:56,556 - INFO - Append list length : 32\n",
      "2021-06-28 21:19:56,558 - INFO - Append list length : 33\n",
      "2021-06-28 21:19:56,558 - INFO - Append list length : 34\n",
      "2021-06-28 21:19:56,559 - INFO - Append list length : 35\n",
      "2021-06-28 21:19:56,565 - INFO - Append list length : 36\n",
      "2021-06-28 21:19:56,566 - INFO - Append list length : 37\n",
      "2021-06-28 21:19:56,571 - INFO - Append list length : 38\n",
      "2021-06-28 21:19:56,572 - INFO - Append list length : 39\n",
      "2021-06-28 21:19:56,573 - INFO - Append list length : 40\n",
      "2021-06-28 21:19:56,574 - INFO - Append list length : 41\n",
      "2021-06-28 21:19:56,580 - INFO - Append list length : 42\n",
      "2021-06-28 21:19:56,581 - INFO - Append list length : 43\n",
      "2021-06-28 21:19:56,582 - INFO - Append list length : 44\n",
      "2021-06-28 21:19:56,585 - INFO - Append list length : 45\n",
      "2021-06-28 21:19:57,612 - INFO - Append list length : 46\n",
      "2021-06-28 21:19:57,621 - INFO - Append list length : 47\n",
      "2021-06-28 21:19:57,629 - INFO - Append list length : 48\n",
      "2021-06-28 21:19:57,632 - INFO - Append list length : 49\n",
      "2021-06-28 21:19:57,633 - INFO - Append list length : 50\n",
      "2021-06-28 21:19:57,638 - INFO - Append list length : 51\n",
      "2021-06-28 21:19:57,639 - INFO - Append list length : 52\n",
      "2021-06-28 21:19:57,640 - INFO - Append list length : 53\n",
      "2021-06-28 21:19:57,641 - INFO - Append list length : 54\n",
      "2021-06-28 21:19:57,642 - INFO - Append list length : 55\n",
      "2021-06-28 21:19:57,644 - INFO - Append list length : 56\n",
      "2021-06-28 21:19:57,645 - INFO - Append list length : 57\n",
      "2021-06-28 21:19:57,646 - INFO - Append list length : 58\n",
      "2021-06-28 21:19:57,647 - INFO - Append list length : 59\n",
      "2021-06-28 21:19:57,656 - INFO - Append list length : 60\n",
      "2021-06-28 21:19:59,558 - INFO - Append list length : 61\n",
      "2021-06-28 21:19:59,561 - INFO - Append list length : 62\n",
      "2021-06-28 21:19:59,563 - INFO - Append list length : 63\n",
      "2021-06-28 21:19:59,566 - INFO - Append list length : 64\n",
      "2021-06-28 21:19:59,567 - INFO - Append list length : 65\n",
      "2021-06-28 21:19:59,568 - INFO - Append list length : 66\n",
      "2021-06-28 21:19:59,569 - INFO - Append list length : 67\n",
      "2021-06-28 21:19:59,570 - INFO - Append list length : 68\n",
      "2021-06-28 21:19:59,575 - INFO - Append list length : 69\n",
      "2021-06-28 21:19:59,580 - INFO - Append list length : 70\n",
      "2021-06-28 21:19:59,581 - INFO - Append list length : 71\n",
      "2021-06-28 21:19:59,582 - INFO - Append list length : 72\n",
      "2021-06-28 21:19:59,582 - INFO - Append list length : 73\n",
      "2021-06-28 21:19:59,583 - INFO - Append list length : 74\n",
      "2021-06-28 21:19:59,583 - INFO - Append list length : 75\n",
      "2021-06-28 21:20:00,643 - INFO - Append list length : 76\n",
      "2021-06-28 21:20:00,651 - INFO - Append list length : 77\n",
      "2021-06-28 21:20:00,653 - INFO - Append list length : 78\n",
      "2021-06-28 21:20:00,655 - INFO - Append list length : 79\n",
      "2021-06-28 21:20:00,656 - INFO - Append list length : 80\n",
      "2021-06-28 21:20:00,664 - INFO - Append list length : 81\n",
      "2021-06-28 21:20:00,665 - INFO - Append list length : 82\n",
      "2021-06-28 21:20:00,668 - INFO - Append list length : 83\n",
      "2021-06-28 21:20:00,669 - INFO - Append list length : 84\n",
      "2021-06-28 21:20:00,670 - INFO - Append list length : 85\n",
      "2021-06-28 21:20:00,671 - INFO - Append list length : 86\n",
      "2021-06-28 21:20:00,672 - INFO - Append list length : 87\n",
      "2021-06-28 21:20:00,672 - INFO - Append list length : 88\n",
      "2021-06-28 21:20:00,675 - INFO - Append list length : 89\n",
      "2021-06-28 21:20:00,676 - INFO - Append list length : 90\n",
      "2021-06-28 21:20:02,704 - INFO - Append list length : 91\n",
      "2021-06-28 21:20:02,710 - INFO - Append list length : 92\n",
      "2021-06-28 21:20:02,723 - INFO - Append list length : 93\n",
      "2021-06-28 21:20:02,729 - INFO - Append list length : 94\n",
      "2021-06-28 21:20:02,736 - INFO - Append list length : 95\n",
      "2021-06-28 21:20:02,743 - INFO - Append list length : 96\n",
      "2021-06-28 21:20:02,744 - INFO - Append list length : 97\n",
      "2021-06-28 21:20:02,745 - INFO - Append list length : 98\n",
      "2021-06-28 21:20:02,747 - INFO - Append list length : 99\n",
      "2021-06-28 21:20:02,751 - INFO - Append list length : 100\n",
      "2021-06-28 21:20:02,752 - INFO - Append list length : 101\n",
      "2021-06-28 21:20:02,753 - INFO - Append list length : 102\n",
      "2021-06-28 21:20:02,754 - INFO - Append list length : 103\n",
      "2021-06-28 21:20:02,755 - INFO - Append list length : 104\n",
      "2021-06-28 21:20:02,756 - INFO - Append list length : 105\n",
      "2021-06-28 21:20:04,652 - INFO - Append list length : 106\n",
      "2021-06-28 21:20:04,658 - INFO - Append list length : 107\n",
      "2021-06-28 21:20:04,664 - INFO - Append list length : 108\n",
      "2021-06-28 21:20:04,669 - INFO - Append list length : 109\n",
      "2021-06-28 21:20:04,670 - INFO - Append list length : 110\n",
      "2021-06-28 21:20:04,677 - INFO - Append list length : 111\n",
      "2021-06-28 21:20:04,679 - INFO - Append list length : 112\n",
      "2021-06-28 21:20:04,681 - INFO - Append list length : 113\n",
      "2021-06-28 21:20:04,682 - INFO - Append list length : 114\n",
      "2021-06-28 21:20:04,683 - INFO - Append list length : 115\n",
      "2021-06-28 21:20:04,684 - INFO - Append list length : 116\n",
      "2021-06-28 21:20:04,685 - INFO - Append list length : 117\n",
      "2021-06-28 21:20:04,692 - INFO - Append list length : 118\n",
      "2021-06-28 21:20:04,693 - INFO - Append list length : 119\n",
      "2021-06-28 21:20:04,693 - INFO - Append list length : 120\n",
      "2021-06-28 21:20:06,561 - INFO - Append list length : 121\n",
      "2021-06-28 21:20:06,568 - INFO - Append list length : 122\n",
      "2021-06-28 21:20:06,574 - INFO - Append list length : 123\n",
      "2021-06-28 21:20:06,577 - INFO - Append list length : 124\n",
      "2021-06-28 21:20:06,578 - INFO - Append list length : 125\n",
      "2021-06-28 21:20:06,580 - INFO - Append list length : 126\n",
      "2021-06-28 21:20:06,584 - INFO - Append list length : 127\n",
      "2021-06-28 21:20:06,586 - INFO - Append list length : 128\n",
      "2021-06-28 21:20:06,587 - INFO - Append list length : 129\n",
      "2021-06-28 21:20:06,588 - INFO - Append list length : 130\n",
      "2021-06-28 21:20:06,589 - INFO - Append list length : 131\n",
      "2021-06-28 21:20:06,592 - INFO - Append list length : 132\n",
      "2021-06-28 21:20:06,592 - INFO - Append list length : 133\n",
      "2021-06-28 21:20:06,593 - INFO - Append list length : 134\n",
      "2021-06-28 21:20:06,594 - INFO - Append list length : 135\n",
      "2021-06-28 21:20:07,541 - INFO - Append list length : 136\n",
      "2021-06-28 21:20:07,542 - INFO - Append list length : 137\n",
      "2021-06-28 21:20:07,543 - INFO - Append list length : 138\n",
      "2021-06-28 21:20:07,545 - INFO - Append list length : 139\n",
      "2021-06-28 21:20:07,548 - INFO - Append list length : 140\n",
      "2021-06-28 21:20:07,549 - INFO - Append list length : 141\n",
      "2021-06-28 21:20:07,550 - INFO - Append list length : 142\n",
      "2021-06-28 21:20:07,550 - INFO - Append list length : 143\n",
      "2021-06-28 21:20:07,551 - INFO - Append list length : 144\n",
      "2021-06-28 21:20:07,552 - INFO - Append list length : 145\n",
      "2021-06-28 21:20:07,554 - INFO - Append list length : 146\n",
      "2021-06-28 21:20:07,555 - INFO - Append list length : 147\n",
      "2021-06-28 21:20:07,558 - INFO - Append list length : 148\n",
      "2021-06-28 21:20:07,559 - INFO - Append list length : 149\n",
      "2021-06-28 21:20:07,560 - INFO - Append list length : 150\n",
      "2021-06-28 21:20:08,517 - INFO - Append list length : 151\n",
      "2021-06-28 21:20:08,518 - INFO - Append list length : 152\n",
      "2021-06-28 21:20:08,519 - INFO - Append list length : 153\n",
      "2021-06-28 21:20:08,522 - INFO - Append list length : 154\n",
      "2021-06-28 21:20:08,529 - INFO - Append list length : 155\n",
      "2021-06-28 21:20:08,531 - INFO - Append list length : 156\n",
      "2021-06-28 21:20:08,532 - INFO - Append list length : 157\n",
      "2021-06-28 21:20:08,535 - INFO - Append list length : 158\n",
      "2021-06-28 21:20:08,538 - INFO - Append list length : 159\n",
      "2021-06-28 21:20:08,540 - INFO - Append list length : 160\n",
      "2021-06-28 21:20:08,541 - INFO - Append list length : 161\n",
      "2021-06-28 21:20:08,542 - INFO - Append list length : 162\n",
      "2021-06-28 21:20:08,543 - INFO - Append list length : 163\n",
      "2021-06-28 21:20:08,543 - INFO - Append list length : 164\n",
      "2021-06-28 21:20:08,544 - INFO - Append list length : 165\n",
      "2021-06-28 21:20:09,519 - INFO - Append list length : 166\n",
      "2021-06-28 21:20:09,539 - INFO - Append list length : 167\n",
      "2021-06-28 21:20:09,545 - INFO - Append list length : 168\n",
      "2021-06-28 21:20:09,549 - INFO - Append list length : 169\n",
      "2021-06-28 21:20:09,553 - INFO - Append list length : 170\n",
      "2021-06-28 21:20:09,554 - INFO - Append list length : 171\n",
      "2021-06-28 21:20:09,560 - INFO - Append list length : 172\n",
      "2021-06-28 21:20:09,561 - INFO - Append list length : 173\n",
      "2021-06-28 21:20:09,562 - INFO - Append list length : 174\n",
      "2021-06-28 21:20:09,563 - INFO - Append list length : 175\n",
      "2021-06-28 21:20:09,566 - INFO - Append list length : 176\n",
      "2021-06-28 21:20:09,568 - INFO - Append list length : 177\n",
      "2021-06-28 21:20:09,586 - INFO - Append list length : 178\n",
      "2021-06-28 21:20:09,587 - INFO - Append list length : 179\n",
      "2021-06-28 21:20:09,588 - INFO - Append list length : 180\n",
      "2021-06-28 21:20:10,386 - INFO - Append list length : 181\n",
      "2021-06-28 21:20:10,394 - INFO - Append list length : 182\n",
      "2021-06-28 21:20:10,400 - INFO - Append list length : 183\n",
      "2021-06-28 21:20:10,401 - INFO - Append list length : 184\n",
      "2021-06-28 21:20:10,402 - INFO - Append list length : 185\n",
      "2021-06-28 21:20:11,116 - INFO - elapsed_time: '29.41827416419983'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "output = []\n",
    "for tweet in tweets_list:\n",
    "    text = tweet._json[\"full_text\"]\n",
    "    #print(text) \n",
    "    # https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets           \n",
    "    # \"geo\": null,\"coordinates\": null,\"place\": null,\"contributors\": null,\n",
    "    # \"is_quote_status\": false,\"retweet_count\": 988,\"favorite_count\": 3875,\n",
    "    # \"favorited\": false,\"retweeted\": false,\"possibly_sensitive\": false,\"lang\": \"en\"\n",
    "    logger.debug(f\"full_text: '{text}'\")\n",
    "    favourite_count = tweet.favorite_count\n",
    "    retweet_count = tweet.retweet_count\n",
    "    created_at = tweet.created_at\n",
    "    \n",
    "    line = {'text' : text, 'favourite_count' : favourite_count, 'retweet_count' : retweet_count, 'created_at' : created_at}\n",
    "    output.append(line)\n",
    "    logger.info(f\"Append list length : { len(output)}\")\n",
    "end = time.time()\n",
    "logger.info(f\"elapsed_time: '{end - start}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 27, 1, 10, 22)},\n",
       " {'text': 'RT @jesse_martin_1: One Piece, Coffee Island #fanart #onepiece #digitalart #luffy #OnePiece1017 https://t.co/QruHqMRmrr',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 27, 0, 27, 26)},\n",
       " {'text': 'All that walking made for thirsty work so we went for a cup of coffee – Ubud Style at the Bali Pulina Plantation. https://t.co/7OapENpCnK #travel',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 27, 0, 21, 21)},\n",
       " {'text': 'Read Treasure Island and enjoy some milk out of a irish coffee glass.',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 27, 0, 6, 4)},\n",
       " {'text': '@magmacumlord literally i say water as wadda. new york is new yoak. long island? lawng eyelailand. coffee/coaffee. shit like that especially. whether they notice it or not theres def an accent and it is HEAVY',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 23, 29, 16)},\n",
       " {'text': 'Apparently one of the women about to ‘star’ in Love Island ‘needs’ sex six times a day.\\n\\nShe clearly doesn’t appreciate the absolute joy of a lovely sit down and a cup of coffee yet. It’ll happen, love.',\n",
       "  'favourite_count': 3,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 23, 10, 27)},\n",
       " {'text': 'RT @UWCNVI: The #Nanaimo 7-10 Club is running a cooling centre at 285 Prideaux Jun 26-Jul 10 supported by @UWCNVI, @cityofnanaimo + @CMHA_M…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 4,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 22, 47, 13)},\n",
       " {'text': 'RT @CatherineSher10: Angel Oak, Southern Live Oak, Johns Island, South Carolina \\n#SouthCarolina\\n#FineArtAmerica #CLS \\n\\n#ArtForSale https://…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 16,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 22, 3, 20)},\n",
       " {'text': 'RT @CatherineSher10: A statue of Hawaiian King Kamehameha the Great, Wailoa State Park in #Hilo #Hawaii on the Big Island. 🌺\\n#BigIsland #CL…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 34,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 22, 3, 2)},\n",
       " {'text': 'RT @cathsherman: #Bahamas Fishing Pier, Great Abaco Island.\\n#GreatAbaco #FineArtAmerica #CLS\\n#ArtForSale \\nAll Products: https://t.co/2BPDbE…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 41,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 21, 43, 4)},\n",
       " {'text': 'RT @cathsherman: #Bahamas Fishing Pier, Great Abaco Island.\\n#GreatAbaco #FineArtAmerica #CLS\\n#ArtForSale \\nAll Products: https://t.co/2BPDbE…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 41,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 21, 40, 57)},\n",
       " {'text': 'RT @cathsherman: #Bahamas Fishing Pier, Great Abaco Island.\\n#GreatAbaco #FineArtAmerica #CLS\\n#ArtForSale \\nAll Products: https://t.co/2BPDbE…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 41,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 21, 20, 28)},\n",
       " {'text': 'RT @jesse_martin_1: One Piece, Coffee Island #fanart #onepiece #digitalart #luffy #OnePiece1017 https://t.co/QruHqMRmrr',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 20, 59, 22)},\n",
       " {'text': 'RT @cathsherman: #Bahamas Fishing Pier, Great Abaco Island.\\n#GreatAbaco #FineArtAmerica #CLS\\n#ArtForSale \\nAll Products: https://t.co/2BPDbE…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 41,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 20, 53, 23)},\n",
       " {'text': 'RT @jesse_martin_1: One Piece, Coffee Island #fanart #onepiece #digitalart #luffy #OnePiece1017 https://t.co/QruHqMRmrr',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 20, 38, 24)},\n",
       " {'text': '@coffee_anytime My view wilmington island ga 🏖 https://t.co/aTS1gPZ7kX',\n",
       "  'favourite_count': 1,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 20, 22, 20)},\n",
       " {'text': 'The way in love island Curtis is with Amy at the start then tells her he wants to make everyone a coffee in the morning AHHHHHH I HATE MEN',\n",
       "  'favourite_count': 1,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 20, 12, 15)},\n",
       " {'text': \"⠀\\n⠀' actually i just really love doing chores, '\\n⠀he jokes, refolding his arms upon the\\n⠀surface of the island. he registers the\\n⠀touch and it makes him look away, but\\n⠀not without a smile as he sips coffee.\\n\\n⠀' why? do you like mine? '\\n⠀ https://t.co/QIfCJFKBpv\",\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 19, 59, 15)},\n",
       " {'text': 'RT @jesse_martin_1: One Piece, Coffee Island #fanart #onepiece #digitalart #luffy #OnePiece1017 https://t.co/QruHqMRmrr',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 19, 15, 30)},\n",
       " {'text': \"Nice coffee flavor blending in with a creamy mouth feel.  @thegarysharp Doesn't know good beer - Drinking an Illuminated Darkness by Pooles Island Brewing Co - https://t.co/ldBh2Vum9z\",\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 19, 10, 59)},\n",
       " {'text': 'RT @jesse_martin_1: One Piece, Coffee Island #fanart #onepiece #digitalart #luffy #OnePiece1017 https://t.co/QruHqMRmrr',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 19, 9, 42)},\n",
       " {'text': 'RT @jesse_martin_1: One Piece, Coffee Island #fanart #onepiece #digitalart #luffy #OnePiece1017 https://t.co/QruHqMRmrr',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 18, 59, 49)},\n",
       " {'text': 'RT @jesse_martin_1: One Piece, Coffee Island #fanart #onepiece #digitalart #luffy #OnePiece1017 https://t.co/QruHqMRmrr',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 18, 46, 52)},\n",
       " {'text': 'i literally went insane  when weston started singing lonely island on the last coffee cam',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 18, 22, 32)},\n",
       " {'text': 'RT @jesse_martin_1: One Piece, Coffee Island #fanart #onepiece #digitalart #luffy #OnePiece1017 https://t.co/QruHqMRmrr',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 18, 19, 56)},\n",
       " {'text': 'RT @SafemoonWarrior: #SAFEMOON🕵🏻\\u200d♂️ \\nCan someone tell if this is a country / island / territory? Probably coffee producer. \\n\\nKarony said so…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 27,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 18, 19, 19)},\n",
       " {'text': 'sometimes I wish I lived on an island where I could start everyday in a local coffee shop with a mocha and a bagel :(',\n",
       "  'favourite_count': 1,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 18, 14, 8)},\n",
       " {'text': 'Yauco is a beautiful colorful city in the southern part of Puerto Rico! During our time here, we got to see some really incredible art in the city known as the \"Coffee Town\" on the island. Yauco got this name as it has been a major coffee hub for a long time. https://t.co/4rx2kPsz0t',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 18, 0, 56)},\n",
       " {'text': 'RT @jesse_martin_1: One Piece, Coffee Island #fanart #onepiece #digitalart #luffy #OnePiece1017 https://t.co/QruHqMRmrr',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 17, 59, 58)},\n",
       " {'text': 'RT @jesse_martin_1: One Piece, Coffee Island #fanart #onepiece #digitalart #luffy #OnePiece1017 https://t.co/QruHqMRmrr',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 17, 48, 35)},\n",
       " {'text': 'i wanna open a coffee shop cause why is there only so few in long island',\n",
       "  'favourite_count': 7,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 17, 11, 38)},\n",
       " {'text': 'RT @jesse_martin_1: One Piece, Coffee Island #fanart #onepiece #digitalart #luffy #OnePiece1017 https://t.co/QruHqMRmrr',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 17, 0, 42)},\n",
       " {'text': 'RT @jesse_martin_1: One Piece, Coffee Island #fanart #onepiece #digitalart #luffy #OnePiece1017 https://t.co/QruHqMRmrr',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 16, 57, 5)},\n",
       " {'text': 'One Piece, Coffee Island #fanart #onepiece #digitalart #luffy #OnePiece1017 https://t.co/QruHqMRmrr',\n",
       "  'favourite_count': 426,\n",
       "  'retweet_count': 84,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 16, 55, 33)},\n",
       " {'text': 'RT @Tara_Hewitt: @buckinghamh @joyfurnival @Hattieinspace @pioneer_retail The Barrow in furness pies sold from houses on the end of terrace…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 1,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 16, 25, 4)},\n",
       " {'text': '@buckinghamh @joyfurnival @Hattieinspace @pioneer_retail The Barrow in furness pies sold from houses on the end of terraced streets where you had to bring a coffee jar to fill with gravy..... also a good shout for top pies (Walney Island Girl and mums family all Barrovians)',\n",
       "  'favourite_count': 6,\n",
       "  'retweet_count': 1,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 16, 24, 48)},\n",
       " {'text': 'RT @cathsherman: Afternoon at the Eighteenth Hole, Kiawah Island, South Carolina #KiawahIsland #Golf ⛳️🏌️  #SouthCarolina Clubhouse \\n#SOLD…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 51,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 15, 44, 34)},\n",
       " {'text': 'RT @UWCNVI: The #Nanaimo 7-10 Club is running a cooling centre at 285 Prideaux Jun 26-Jul 10 supported by @UWCNVI, @cityofnanaimo + @CMHA_M…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 4,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 15, 34, 14)},\n",
       " {'text': 'RT @UWCNVI: The #Nanaimo 7-10 Club is running a cooling centre at 285 Prideaux Jun 26-Jul 10 supported by @UWCNVI, @cityofnanaimo + @CMHA_M…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 4,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 15, 31, 25)},\n",
       " {'text': '2.) — inalienable part of the People’s Republic. Beijing has underscored the need for Taiwan – the island it considers to be an integral part of China – to acknowledge that its future lies in “reunification”. Weighing in on last Tuesday’s incident that saw 28 Chinese air force —',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 14, 56, 23)},\n",
       " {'text': \"***Very Strawberry &amp; Blue Cotton Candy***\\nChocolate Ice Cream &amp; Vanilla Ice Cream\\nNSA Vanilla &amp; Campfire Crush\\nNew York Cheesecake &amp; Pineapple Dole Soft Serve\\nCold Brew Coffee Gelato &amp; Cake Batter\\nSweet Coconut &amp; Island Banana\\nCookie n' Cream &amp; Caramel Sea Salt Gelato https://t.co/qT474mhoyy\",\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 14, 27, 32)},\n",
       " {'text': 'LIVE!\\n\\nGood morning. Just made some coffee, stirred in a little creamer with this tiny cat spoon then proceeded to get yelled at by my cat.\\n\\nWorking more on our new island, see you in there!\\n\\nhttps://t.co/zkERJII1iE https://t.co/Qb8lUYgm6W',\n",
       "  'favourite_count': 3,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 14, 7, 13)},\n",
       " {'text': 'Funny how a song can transport you. Vanessa Carlton’s on the radio and I’m 20 again just starting my J1 visa working in the National hotel on Block Island, USA, currently on coffee break in a cafe https://t.co/s9SyKQucda',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 13, 56, 29)},\n",
       " {'text': 'RT @cathsherman: #Bahamas Fishing Pier, Great Abaco Island.\\n#GreatAbaco #FineArtAmerica #CLS\\n#ArtForSale \\nAll Products: https://t.co/2BPDbE…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 41,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 13, 34, 8)},\n",
       " {'text': 'Since we can’t travel to Canary Island we brought Canaries to Evuna..\\n\\nBarraquito Coffee..\\n\\nLayers of Liquour 43, condensed milk, coffee and milk finished off with cinnamon and lemon zest. https://t.co/PFFUb59TAq',\n",
       "  'favourite_count': 3,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 13, 30, 22)},\n",
       " {'text': 'Read Treasure Island and drink some chilled water from a irish coffee glass.',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 13, 25, 26)},\n",
       " {'text': 'Picked this up in a little coffee shop on a tropical island in the pacific. Memorable vacation, that one was. #JurassicWorld #JurassicPark #JurassicWorldDominion https://t.co/0bxBJE1CNU',\n",
       "  'favourite_count': 2,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 13, 9, 2)},\n",
       " {'text': 'Coney Island restaurant for breakfast &amp; coffee',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 12, 0, 36)},\n",
       " {'text': 'and jennie got tired of moving from one country to another just to keep a safe distance away from the problematic family business and went to jeju island where she stayed at a famous coffee shop owned by jisoo’s grandfather\\n\\nwow. i’m totally bored',\n",
       "  'favourite_count': 1,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 11, 5, 23)},\n",
       " {'text': 'RT @mir_gifs: June 19th bunny and animal crossing island coffee status ☕️🐰🐶 https://t.co/cHdGkmEMN1',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 5,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 11, 4, 16)},\n",
       " {'text': \"Good Morning, I've got the coffee on for you.\",\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 11, 0, 3)},\n",
       " {'text': 'i wanna move to staten island and get a cat and open a coffee shop 👍',\n",
       "  'favourite_count': 1,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 10, 13, 24)},\n",
       " {'text': 'On the 2nd anniversary of the day I planned to die in going to Majorca. I need these things to look forward to. Little things like coffee with a friend, a curry night... Bigger things, like getting on a plan and leaving this shit little island for 10 days.',\n",
       "  'favourite_count': 14,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 10, 12, 53)},\n",
       " {'text': \"@need_coffee_now Proper job, Stuart. This sort of jingoism that musn't take root on this island.\",\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 10, 6, 41)},\n",
       " {'text': '@UnknowingTitan [Groans into the floor.] Hanji gave me this weird coffee. We ran a few miles, I did a bunch of cartwheels in my titan, and then I think I fell off the wall after I climbed halfway up. Hanji cut my arm off getting me out. I feel like every horse on the island trampled me, twice.',\n",
       "  'favourite_count': 1,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 9, 37, 3)},\n",
       " {'text': 'A nice latte in Rhode Island coffee! https://t.co/6vRDOsXsK5',\n",
       "  'favourite_count': 2,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 9, 26, 44)},\n",
       " {'text': '@TAHK0 Still mad at island limitations per console, ngl. My partner still loves it tho so cant be too mad.',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 9, 17, 1)},\n",
       " {'text': '🥥 Imagine a Bounty dunked in coffee... Subtle hints of coconut adds a tropical twist to our smooth Arabica coffee and will have you on island time in no time 🏝 \\n\\n#nationalcoconutday #livenupyourcup #coconutcoffee #flavouredcoffee https://t.co/abvWkRtjLk',\n",
       "  'favourite_count': 1,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 8, 30, 42)},\n",
       " {'text': 'In the bakers in town. Best coffee on the island \\nSteely Dan on in background \\nWhat a time to be alive https://t.co/U5UZ5J8lVf',\n",
       "  'favourite_count': 8,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 7, 51, 26)},\n",
       " {'text': 'RT @VectisRadio: Join Maggie for the Breakfast Show, sponsored by Island Environmental Hygiene Ltd, from 8am.  Hear great music, local musi…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 2,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 6, 20, 51)},\n",
       " {'text': 'RT @VectisRadio: Join Maggie for the Breakfast Show, sponsored by Island Environmental Hygiene Ltd, from 8am.  Hear great music, local musi…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 2,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 6, 20, 46)},\n",
       " {'text': 'Join Maggie for the Breakfast Show, sponsored by Island Environmental Hygiene Ltd, from 8am.  Hear great music, local music from Glenn Koppany and Bethan John Music, a motivational moment, how to join our virtual coffee morning and much more.  See you there, https://t.co/T09RR5pbrW',\n",
       "  'favourite_count': 2,\n",
       "  'retweet_count': 2,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 6, 20, 37)},\n",
       " {'text': 'today I heard a woman at fashion island refer to Arabic coffee as a “coffee shot” and I want to die',\n",
       "  'favourite_count': 3,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 5, 54, 13)},\n",
       " {'text': 'RT @cathsherman: #Bahamas Fishing Pier, Great Abaco Island.\\n#GreatAbaco #FineArtAmerica #CLS\\n#ArtForSale \\nAll Products: https://t.co/2BPDbE…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 41,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 4, 50, 16)},\n",
       " {'text': 'Lately been obsessed over aquariums in unorthodox places like kitchen island, a bar top, coffee tables.\\n\\nI want aquariums in every room in the house',\n",
       "  'favourite_count': 1,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 4, 49, 30)},\n",
       " {'text': 'RT @UWCNVI: The #Nanaimo 7-10 Club is running a cooling centre at 285 Prideaux Jun 26-Jul 10 supported by @UWCNVI, @cityofnanaimo + @CMHA_M…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 4,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 4, 35, 16)},\n",
       " {'text': 'RT @visit_kochi: Kashiwa is located at the southwestern tip of Kochi, and boasts waters so clear that boats look like they are suspended in…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 4,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 3, 45, 5)},\n",
       " {'text': '@jeremarketer You won’t regret it, if you can score some 100% Kona coffee it’s like no other. Order direct from the Big Island of Hawaii',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 3, 34, 28)},\n",
       " {'text': 'RT @CatherineSher10: A statue of Hawaiian King Kamehameha the Great, Wailoa State Park in #Hilo #Hawaii on the Big Island. 🌺\\n#BigIsland #CL…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 34,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 3, 22, 54)},\n",
       " {'text': 'RT @CatherineSher10: A statue of Hawaiian King Kamehameha the Great, Wailoa State Park in #Hilo #Hawaii on the Big Island. 🌺\\n#BigIsland #CL…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 34,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 3, 20, 48)},\n",
       " {'text': 'RT @cathsherman: #Bahamas Fishing Pier, Great Abaco Island.\\n#GreatAbaco #FineArtAmerica #CLS\\n#ArtForSale \\nAll Products: https://t.co/2BPDbE…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 41,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 3, 18, 36)},\n",
       " {'text': \"@UnclePlasma Yes that's right. And I found it out to be both actually. The day I figured that out we had a game at Battin Island, which happened to be in coffee weather. When I sat down to watch the game with a cup of coffee, thats when he reached out to me, as a face in the cup.\",\n",
       "  'favourite_count': 3,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 1, 53, 43)},\n",
       " {'text': 'Good Morning\\n\\nGood Coffee makes the day happier @ Golf Island PIK https://t.co/ydDc4ulXqk',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 0,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 0, 50, 2)},\n",
       " {'text': 'RT @SafemoonWarrior: #SAFEMOON🕵🏻\\u200d♂️ \\nCan someone tell if this is a country / island / territory? Probably coffee producer. \\n\\nKarony said so…',\n",
       "  'favourite_count': 0,\n",
       "  'retweet_count': 27,\n",
       "  'created_at': datetime.datetime(2021, 6, 26, 0, 42, 5)}]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "source": [
    "---\n",
    "### create sdf from list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert the `output` list to a `spark DataFrame` and we store results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/spark/python/pyspark/sql/session.py:378: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n",
      "+-------------------+---------------+-------------+------------------------------+\n",
      "|         created_at|favourite_count|retweet_count|                          text|\n",
      "+-------------------+---------------+-------------+------------------------------+\n",
      "|2021-06-27 23:45:08|              1|            1|Lava Lei’s signature 100% K...|\n",
      "|2021-06-27 23:38:04|              0|            0|@johnpavlovitz You might ha...|\n",
      "|2021-06-27 22:35:32|             16|            1|I wonder if villagers walk ...|\n",
      "|2021-06-27 22:14:58|              2|            0|@parislord @FeathersOz @the...|\n",
      "|2021-06-27 22:07:46|              0|            0|@Hold2LLC @TheEliKlein I'm ...|\n",
      "|2021-06-27 22:00:07|              2|            1|. @WickWrites loves to trav...|\n",
      "|2021-06-27 21:38:33|              0|            0|@catekitchen OH is the cake...|\n",
      "|2021-06-27 21:21:35|              7|            1|Ok I’ve had enough of peopl...|\n",
      "|2021-06-27 20:10:09|              0|            0|Island bound 😁\n",
      "\n",
      " #coffeesh...|\n",
      "|2021-06-27 19:45:33|              8|            0|Tweet tweet… had a little v...|\n",
      "|2021-06-27 19:32:36|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 17:49:05|              1|            0|@ambrogaming92 Buy you a ch...|\n",
      "|2021-06-27 17:37:27|              2|            0|I’m so loud ab my hatred fo...|\n",
      "|2021-06-27 17:36:03|              0|            0|Read Treasure Island and dr...|\n",
      "|2021-06-27 17:24:46|              0|            0|@lovexmae Girrrrrrllll you ...|\n",
      "|2021-06-27 17:24:26|              3|            0|@Fibncci011235 @bimbouberme...|\n",
      "|2021-06-27 17:10:19|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 17:06:36|              1|            0|@Saltygoat5 1/\n",
      "\n",
      "...and anot...|\n",
      "|2021-06-27 16:57:08|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 16:50:17|              0|            0|Now available: Tröegs Nimbl...|\n",
      "|2021-06-27 16:29:19|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 16:12:58|             11|            0|Last day on the island - I ...|\n",
      "|2021-06-27 16:04:08|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 15:54:25|             20|            0|Meet 'pre-M.E Sally'.\n",
      "\n",
      "Than...|\n",
      "|2021-06-27 15:48:31|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 15:28:03|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 15:26:00|              1|            0|@Seinfeldism1 I’m triggered...|\n",
      "|2021-06-27 15:23:13|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 15:15:50|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 15:12:15|              0|           34|RT @CatherineSher10: A stat...|\n",
      "|2021-06-27 15:02:05|             11|            0|Sweating my ass off sitting...|\n",
      "|2021-06-27 14:58:02|              0|            0|The perfect way to start th...|\n",
      "|2021-06-27 14:57:45|              0|           25|RT @visitcyprus: Morning co...|\n",
      "|2021-06-27 14:25:06|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 13:55:19|              0|           41|RT @cathsherman: #Bahamas F...|\n",
      "|2021-06-27 13:54:00|              0|           34|RT @CatherineSher10: A stat...|\n",
      "|2021-06-27 13:51:42|              0|           34|RT @CatherineSher10: A stat...|\n",
      "|2021-06-27 13:49:47|              0|            0|went to private island.. bi...|\n",
      "|2021-06-27 13:39:07|              0|           54|RT @acnh_luna: Dreamt of th...|\n",
      "|2021-06-27 13:38:17|              0|           52|RT @MallowACNH: Star Falls ...|\n",
      "|2021-06-27 13:26:31|              0|            0|There's coffee, and there i...|\n",
      "|2021-06-27 13:24:37|              0|           19|RT @Iuxprima: open the ligh...|\n",
      "|2021-06-27 13:14:20|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 13:06:16|              0|            3|RT @liam_mchugh: First Sund...|\n",
      "|2021-06-27 13:01:12|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 12:53:51|              0|            3|RT @liam_mchugh: First Sund...|\n",
      "|2021-06-27 12:51:10|              0|            3|RT @liam_mchugh: First Sund...|\n",
      "|2021-06-27 12:47:44|              2|            0|Chilling poolside with Grog...|\n",
      "|2021-06-27 12:47:39|             72|            3|First Sunday since school e...|\n",
      "|2021-06-27 12:43:46|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 12:39:30|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 12:28:40|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 12:14:19|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 12:13:01|              2|            0|To think, two years ago tod...|\n",
      "|2021-06-27 11:58:40|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 11:51:38|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 11:50:52|              3|            0|aight who wants to move to ...|\n",
      "|2021-06-27 11:49:01|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 11:47:32|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 11:47:19|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 11:25:27|              0|            0|The taste of freeness hits ...|\n",
      "|2021-06-27 11:13:06|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 11:09:44|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 11:00:35|              0|            0|Good Morning, I've got the ...|\n",
      "|2021-06-27 10:47:11|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 10:47:08|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 10:22:09|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 10:06:31|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 09:59:13|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 09:42:51|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 09:19:09|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 09:14:18|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 09:08:20|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 09:06:20|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 09:01:00|              0|            4|RT @InciTUNA7: Good Morning...|\n",
      "|2021-06-27 08:55:55|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 08:48:27|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 08:29:54|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 08:08:17|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 08:04:36|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 07:59:33|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 07:55:08|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 07:54:07|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 07:48:42|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 07:48:01|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 07:47:34|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 07:03:36|              0|            4|RT @InciTUNA7: Good Morning...|\n",
      "|2021-06-27 06:47:39|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 06:12:15|              1|            0|@TrekkieRN_NP Coffee shop m...|\n",
      "|2021-06-27 05:51:31|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 05:39:40|              0|           27|RT @SafemoonWarrior: #SAFEM...|\n",
      "|2021-06-27 05:33:42|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 05:11:07|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 05:08:36|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 05:08:31|              1|            0|beach hopping to speciality...|\n",
      "|2021-06-27 05:02:56|              0|           41|RT @cathsherman: #Bahamas F...|\n",
      "|2021-06-27 04:33:35|              7|            0|@freshwaterpurl With books ...|\n",
      "|2021-06-27 04:28:01|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 04:19:11|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 03:52:24|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 03:33:45|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 03:33:28|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 03:32:09|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 02:51:19|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 02:50:12|              2|            0|@ThebanMonk @Ty_in_TX @El__...|\n",
      "|2021-06-27 02:43:21|              1|            0|@coffee_anytime Ohhh yeah!!...|\n",
      "|2021-06-27 02:40:41|              1|            0|@sassyausgirl Me, too.  How...|\n",
      "|2021-06-27 02:34:23|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 02:21:02|              0|            0|Hawai'i Coffee Association ...|\n",
      "|2021-06-27 02:10:17|              1|            0|@jtc4free @bflewis3 @GarySh...|\n",
      "|2021-06-27 01:57:35|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 01:10:22|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 00:27:26|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-27 00:21:21|              0|            0|All that walking made for t...|\n",
      "|2021-06-27 00:06:04|              0|            0|Read Treasure Island and en...|\n",
      "|2021-06-26 23:29:16|              0|            0|@magmacumlord literally i s...|\n",
      "|2021-06-26 23:10:27|              3|            0|Apparently one of the women...|\n",
      "|2021-06-26 22:47:13|              0|            4|RT @UWCNVI: The #Nanaimo 7-...|\n",
      "|2021-06-26 22:03:20|              0|           16|RT @CatherineSher10: Angel ...|\n",
      "|2021-06-26 22:03:02|              0|           34|RT @CatherineSher10: A stat...|\n",
      "|2021-06-26 21:43:04|              0|           41|RT @cathsherman: #Bahamas F...|\n",
      "|2021-06-26 21:40:57|              0|           41|RT @cathsherman: #Bahamas F...|\n",
      "|2021-06-26 21:20:28|              0|           41|RT @cathsherman: #Bahamas F...|\n",
      "|2021-06-26 20:59:22|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-26 20:53:23|              0|           41|RT @cathsherman: #Bahamas F...|\n",
      "|2021-06-26 20:38:24|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-26 20:22:20|              1|            0|@coffee_anytime My view wil...|\n",
      "|2021-06-26 20:12:15|              1|            0|The way in love island Curt...|\n",
      "|2021-06-26 19:59:15|              0|            0|⠀\n",
      "⠀' actually i just really...|\n",
      "|2021-06-26 19:15:30|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-26 19:10:59|              0|            0|Nice coffee flavor blending...|\n",
      "|2021-06-26 19:09:42|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-26 18:59:49|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-26 18:46:52|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-26 18:22:32|              0|            0|i literally went insane  wh...|\n",
      "|2021-06-26 18:19:56|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-26 18:19:19|              0|           27|RT @SafemoonWarrior: #SAFEM...|\n",
      "|2021-06-26 18:14:08|              1|            0|sometimes I wish I lived on...|\n",
      "|2021-06-26 18:00:56|              0|            0|Yauco is a beautiful colorf...|\n",
      "|2021-06-26 17:59:58|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-26 17:48:35|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-26 17:11:38|              7|            0|i wanna open a coffee shop ...|\n",
      "|2021-06-26 17:00:42|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-26 16:57:05|              0|           84|RT @jesse_martin_1: One Pie...|\n",
      "|2021-06-26 16:55:33|            426|           84|One Piece, Coffee Island #f...|\n",
      "|2021-06-26 16:25:04|              0|            1|RT @Tara_Hewitt: @buckingha...|\n",
      "|2021-06-26 16:24:48|              6|            1|@buckinghamh @joyfurnival @...|\n",
      "|2021-06-26 15:44:34|              0|           51|RT @cathsherman: Afternoon ...|\n",
      "|2021-06-26 15:34:14|              0|            4|RT @UWCNVI: The #Nanaimo 7-...|\n",
      "|2021-06-26 15:31:25|              0|            4|RT @UWCNVI: The #Nanaimo 7-...|\n",
      "|2021-06-26 14:56:23|              0|            0|2.) — inalienable part of t...|\n",
      "|2021-06-26 14:27:32|              0|            0|***Very Strawberry &amp; Bl...|\n",
      "|2021-06-26 14:07:13|              3|            0|LIVE!\n",
      "\n",
      "Good morning. Just m...|\n",
      "|2021-06-26 13:56:29|              0|            0|Funny how a song can transp...|\n",
      "|2021-06-26 13:34:08|              0|           41|RT @cathsherman: #Bahamas F...|\n",
      "|2021-06-26 13:30:22|              3|            0|Since we can’t travel to Ca...|\n",
      "|2021-06-26 13:25:26|              0|            0|Read Treasure Island and dr...|\n",
      "|2021-06-26 13:09:02|              2|            0|Picked this up in a little ...|\n",
      "|2021-06-26 12:00:36|              0|            0|Coney Island restaurant for...|\n",
      "|2021-06-26 11:05:23|              1|            0|and jennie got tired of mov...|\n",
      "|2021-06-26 11:04:16|              0|            5|RT @mir_gifs: June 19th bun...|\n",
      "|2021-06-26 11:00:03|              0|            0|Good Morning, I've got the ...|\n",
      "|2021-06-26 10:13:24|              1|            0|i wanna move to staten isla...|\n",
      "|2021-06-26 10:12:53|             14|            0|On the 2nd anniversary of t...|\n",
      "|2021-06-26 10:06:41|              0|            0|@need_coffee_now Proper job...|\n",
      "|2021-06-26 09:37:03|              1|            0|@UnknowingTitan [Groans int...|\n",
      "|2021-06-26 09:26:44|              2|            0|A nice latte in Rhode Islan...|\n",
      "|2021-06-26 09:17:01|              0|            0|@TAHK0 Still mad at island ...|\n",
      "|2021-06-26 08:30:42|              1|            0|🥥 Imagine a Bounty dunked ...|\n",
      "|2021-06-26 07:51:26|              8|            0|In the bakers in town. Best...|\n",
      "|2021-06-26 06:20:51|              0|            2|RT @VectisRadio: Join Maggi...|\n",
      "|2021-06-26 06:20:46|              0|            2|RT @VectisRadio: Join Maggi...|\n",
      "|2021-06-26 06:20:37|              2|            2|Join Maggie for the Breakfa...|\n",
      "|2021-06-26 05:54:13|              3|            0|today I heard a woman at fa...|\n",
      "|2021-06-26 04:50:16|              0|           41|RT @cathsherman: #Bahamas F...|\n",
      "|2021-06-26 04:49:30|              1|            0|Lately been obsessed over a...|\n",
      "|2021-06-26 04:35:16|              0|            4|RT @UWCNVI: The #Nanaimo 7-...|\n",
      "|2021-06-26 03:45:05|              0|            4|RT @visit_kochi: Kashiwa is...|\n",
      "|2021-06-26 03:34:28|              0|            0|@jeremarketer You won’t reg...|\n",
      "|2021-06-26 03:22:54|              0|           34|RT @CatherineSher10: A stat...|\n",
      "|2021-06-26 03:20:48|              0|           34|RT @CatherineSher10: A stat...|\n",
      "|2021-06-26 03:18:36|              0|           41|RT @cathsherman: #Bahamas F...|\n",
      "|2021-06-26 01:53:43|              3|            0|@UnclePlasma Yes that's rig...|\n",
      "|2021-06-26 00:50:02|              0|            0|Good Morning\n",
      "\n",
      "Good Coffee m...|\n",
      "|2021-06-26 00:42:05|              0|           27|RT @SafemoonWarrior: #SAFEM...|\n",
      "+-------------------+---------------+-------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.createDataFrame(output)\n",
    "sdf.show(200, truncate = 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"fields\":[{\"metadata\":{},\"name\":\"created_at\",\"nullable\":true,\"type\":\"timestamp\"},{\"metadata\":{},\"name\":\"favourite_count\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"retweet_count\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"text\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+---------------+-------------+------------------------------+\n|         created_at|favourite_count|retweet_count|                          text|\n+-------------------+---------------+-------------+------------------------------+\n|2021-06-27 23:45:08|              1|            1|Lava Lei’s signature 100% K...|\n|2021-06-27 23:38:04|              0|            0|@johnpavlovitz You might ha...|\n+-------------------+---------------+-------------+------------------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "sdf.show(2, truncate =30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select('text', 'favourite_count', 'retweet_count', 'created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  favourite_count  \\\n",
       "0  Lava Lei’s signature 100% Kona Coffee featurin...                1   \n",
       "1  @johnpavlovitz You might have already passed t...                0   \n",
       "2  I wonder if villagers walk round the island an...               16   \n",
       "\n",
       "   retweet_count          created_at  \n",
       "0              1 2021-06-27 23:45:08  \n",
       "1              0 2021-06-27 23:38:04  \n",
       "2              1 2021-06-27 22:35:32  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>favourite_count</th>\n      <th>retweet_count</th>\n      <th>created_at</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Lava Lei’s signature 100% Kona Coffee featurin...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2021-06-27 23:45:08</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@johnpavlovitz You might have already passed t...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2021-06-27 23:38:04</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I wonder if villagers walk round the island an...</td>\n      <td>16</td>\n      <td>1</td>\n      <td>2021-06-27 22:35:32</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "sdf.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------+---------------+-------------+-------------------+\n|                          text|favourite_count|retweet_count|         created_at|\n+------------------------------+---------------+-------------+-------------------+\n|Lava Lei’s signature 100% K...|              1|            1|2021-06-27 23:45:08|\n|@johnpavlovitz You might ha...|              0|            0|2021-06-27 23:38:04|\n|I wonder if villagers walk ...|             16|            1|2021-06-27 22:35:32|\n+------------------------------+---------------+-------------+-------------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Extract First N rows in pyspark – Top N rows in pyspark using show() function\n",
    "sdf.show(3, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----------------------+\n",
      "|id |input                  |\n",
      "+---+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |\n",
      "|2  |03-01-2019 12 01 19 406|\n",
      "|3  |03-01-2021 12 01 19 406|\n",
      "+---+-----------------------+\n",
      "\n",
      "+---+-----------------------+------------+-----------------------+\n",
      "|id |input                  |current_date|current_timestamp      |\n",
      "+---+-----------------------+------------+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |2021-06-28  |2021-06-28 21:24:40.028|\n",
      "|2  |03-01-2019 12 01 19 406|2021-06-28  |2021-06-28 21:24:40.028|\n",
      "|3  |03-01-2021 12 01 19 406|2021-06-28  |2021-06-28 21:24:40.028|\n",
      "+---+-----------------------+------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df2.show(truncate=False)\n",
    "\n",
    "df2.withColumn(\"current_date\",current_date()) \\\n",
    "  .withColumn(\"current_timestamp\",current_timestamp()) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------+\n|current_timestamp     |\n+----------------------+\n|2021-06-28 21:25:15.97|\n|2021-06-28 21:25:15.97|\n|2021-06-28 21:25:15.97|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.select(current_timestamp().alias(\"current_timestamp\")\n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "source": [
    "---\n",
    "### save and read sdf without header\n",
    "\n",
    " Spark DataFrameWriter class provides a method csv() to save or write a DataFrame at a specified path on disk, this method takes a file path where you wanted to write a file and by default, it doesn’t write a header or column names.\n",
    "\n",
    " #### overwrite mode"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.write.format('csv').mode('overwrite').save('output_cof_island_sdf.csv', escape = '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "part-00000-6768db96-bdff-4212-8d65-7d462a9b10be-c000.csv  _SUCCESS\npart-00001-6768db96-bdff-4212-8d65-7d462a9b10be-c000.csv\n"
     ]
    }
   ],
   "source": [
    "%ls output_cof_island_sdf.csv"
   ]
  },
  {
   "source": [
    "### append mode"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf.write.csv('output_cof_island_sdf.csv', mode = 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, StringType, TimestampType\n",
    "# ['text','favourite_count',\t'retweet_count','created_at']\n",
    "# DateType default format is yyyy-MM-dd \n",
    "# TimestampType default format is yyyy-MM-dd HH:mm:ss.SSSS\n",
    "schema = StructType([\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"favourite_count\", IntegerType(), True),\n",
    "    StructField(\"retweet_count\", IntegerType(), True),\n",
    "    StructField(\"created_at\", TimestampType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "sdf_2 = spark.read.csv('output_cof_island_sdf.csv',schema=schema, header=False,escape = '\"', multiLine=True) #, escape = '\"',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                  text  favourite_count  \\\n",
       "0    RT @jesse_martin_1: One Piece, Coffee Island #...                0   \n",
       "1    RT @jesse_martin_1: One Piece, Coffee Island #...                0   \n",
       "2    beach hopping to speciality coffee shop hoppin...                1   \n",
       "3    RT @cathsherman: #Bahamas Fishing Pier, Great ...                0   \n",
       "4    @freshwaterpurl With books coffee and cinema, ...                7   \n",
       "..                                                 ...              ...   \n",
       "180  RT @jesse_martin_1: One Piece, Coffee Island #...                0   \n",
       "181  @TrekkieRN_NP Coffee shop might be good!  But ...                1   \n",
       "182  RT @jesse_martin_1: One Piece, Coffee Island #...                0   \n",
       "183  RT @SafemoonWarrior: #SAFEMOON🕵🏻‍♂️ \\nCan some...                0   \n",
       "184  RT @jesse_martin_1: One Piece, Coffee Island #...                0   \n",
       "\n",
       "     retweet_count          created_at  \n",
       "0               84 2021-06-27 05:11:07  \n",
       "1               84 2021-06-27 05:08:36  \n",
       "2                0 2021-06-27 05:08:31  \n",
       "3               41 2021-06-27 05:02:56  \n",
       "4                0 2021-06-27 04:33:35  \n",
       "..             ...                 ...  \n",
       "180             84 2021-06-27 06:47:39  \n",
       "181              0 2021-06-27 06:12:15  \n",
       "182             84 2021-06-27 05:51:31  \n",
       "183             27 2021-06-27 05:39:40  \n",
       "184             84 2021-06-27 05:33:42  \n",
       "\n",
       "[185 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>favourite_count</th>\n      <th>retweet_count</th>\n      <th>created_at</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>RT @jesse_martin_1: One Piece, Coffee Island #...</td>\n      <td>0</td>\n      <td>84</td>\n      <td>2021-06-27 05:11:07</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>RT @jesse_martin_1: One Piece, Coffee Island #...</td>\n      <td>0</td>\n      <td>84</td>\n      <td>2021-06-27 05:08:36</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>beach hopping to speciality coffee shop hoppin...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2021-06-27 05:08:31</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>RT @cathsherman: #Bahamas Fishing Pier, Great ...</td>\n      <td>0</td>\n      <td>41</td>\n      <td>2021-06-27 05:02:56</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@freshwaterpurl With books coffee and cinema, ...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>2021-06-27 04:33:35</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>RT @jesse_martin_1: One Piece, Coffee Island #...</td>\n      <td>0</td>\n      <td>84</td>\n      <td>2021-06-27 06:47:39</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>@TrekkieRN_NP Coffee shop might be good!  But ...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2021-06-27 06:12:15</td>\n    </tr>\n    <tr>\n      <th>182</th>\n      <td>RT @jesse_martin_1: One Piece, Coffee Island #...</td>\n      <td>0</td>\n      <td>84</td>\n      <td>2021-06-27 05:51:31</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>RT @SafemoonWarrior: #SAFEMOON🕵🏻‍♂️ \\nCan some...</td>\n      <td>0</td>\n      <td>27</td>\n      <td>2021-06-27 05:39:40</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>RT @jesse_martin_1: One Piece, Coffee Island #...</td>\n      <td>0</td>\n      <td>84</td>\n      <td>2021-06-27 05:33:42</td>\n    </tr>\n  </tbody>\n</table>\n<p>185 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# sdf.show(2, truncate =30)\n",
    "sdf_2.limit(200).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------+---------------+-------------+-------------------+\n|                          text|favourite_count|retweet_count|         created_at|\n+------------------------------+---------------+-------------+-------------------+\n|RT @SafemoonWarrior: #SAFEM...|              0|           27|2021-06-26 00:42:05|\n|Good Morning\n\nGood Coffee m...|              0|            0|2021-06-26 00:50:02|\n+------------------------------+---------------+-------------+-------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# sdf_2 = sdf_2.sort(F.col('created_at'), ascending=True)\n",
    "sdf_2 = sdf_2.sort(F.col('created_at').asc())\n",
    "sdf_2.show(2,truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------+---------------+-------------+-------------------+\n|                          text|favourite_count|retweet_count|         created_at|\n+------------------------------+---------------+-------------+-------------------+\n|Lava Lei’s signature 100% K...|              1|            1|2021-06-27 23:45:08|\n|@johnpavlovitz You might ha...|              0|            0|2021-06-27 23:38:04|\n+------------------------------+---------------+-------------+-------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# sdf_2 = sdf_2.sort(F.col('created_at').desc())\n",
    "sdf_2 = sdf_2.sort(F.col('created_at'), ascending=False)\n",
    "sdf_2.show(2,truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(text='Lava Lei’s signature 100% Kona Coffee featuring Kona beans grown exclusively on the Big Island of Hawaii is calling your name. To shop, visit https://t.co/BE9AQuLTbQ! ☕🌴☀️ #bayviewfarm https://t.co/Z58mxSAR38', favourite_count=1, retweet_count=1, created_at=datetime.datetime(2021, 6, 27, 23, 45, 8)),\n",
       " Row(text='@johnpavlovitz You might have already passed the point of no return on acquiring the taste for it If you’ve never had it, I would not be surprised if you found it incredibly bitter \\n\\nIf had to live on a deserted island and could only bring a few things with me, coffee would be the first grab', favourite_count=0, retweet_count=0, created_at=datetime.datetime(2021, 6, 27, 23, 38, 4))]"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "# Get First N rows in pyspark – Top N rows in pyspark using head() \n",
    "# function – (First 10 rows)\n",
    "sdf_2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Row(text='Lava Lei’s signature 100% Kona Coffee featuring Kona beans grown exclusively on the Big Island of Hawaii is calling your name. To shop, visit https://t.co/BE9AQuLTbQ! ☕🌴☀️ #bayviewfarm https://t.co/Z58mxSAR38', favourite_count=1, retweet_count=1, created_at=datetime.datetime(2021, 6, 27, 23, 45, 8))"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# Extract First row of dataframe in pyspark – using first() function\n",
    "sdf_2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 6, 27, 23, 45, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "sdf_2.select(col(\"created_at\")).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  favourite_count  \\\n",
       "0  Lava Lei’s signature 100% Kona Coffee featurin...                1   \n",
       "1  @johnpavlovitz You might have already passed t...                0   \n",
       "\n",
       "   retweet_count          created_at  \n",
       "0              1 2021-06-27 23:45:08  \n",
       "1              0 2021-06-27 23:38:04  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>favourite_count</th>\n      <th>retweet_count</th>\n      <th>created_at</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Lava Lei’s signature 100% Kona Coffee featurin...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2021-06-27 23:45:08</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@johnpavlovitz You might have already passed t...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2021-06-27 23:38:04</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "sdf_2.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  favourite_count  \\\n",
       "0  RT @SafemoonWarrior: #SAFEMOON🕵🏻‍♂️ \\nCan some...                0   \n",
       "1  Good Morning\\n\\nGood Coffee makes the day happ...                0   \n",
       "\n",
       "   retweet_count          created_at  \n",
       "0             27 2021-06-26 00:42:05  \n",
       "1              0 2021-06-26 00:50:02  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>favourite_count</th>\n      <th>retweet_count</th>\n      <th>created_at</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>RT @SafemoonWarrior: #SAFEMOON🕵🏻‍♂️ \\nCan some...</td>\n      <td>0</td>\n      <td>27</td>\n      <td>2021-06-26 00:42:05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Good Morning\\n\\nGood Coffee makes the day happ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2021-06-26 00:50:02</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "sdf_2.sort(F.col('created_at'), ascending=True).limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                  text  favourite_count  \\\n",
       "183  Good Morning\\n\\nGood Coffee makes the day happ...                0   \n",
       "184  RT @SafemoonWarrior: #SAFEMOON🕵🏻‍♂️ \\nCan some...                0   \n",
       "\n",
       "     retweet_count          created_at  \n",
       "183              0 2021-06-26 00:50:02  \n",
       "184             27 2021-06-26 00:42:05  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>favourite_count</th>\n      <th>retweet_count</th>\n      <th>created_at</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>183</th>\n      <td>Good Morning\\n\\nGood Coffee makes the day happ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2021-06-26 00:50:02</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>RT @SafemoonWarrior: #SAFEMOON🕵🏻‍♂️ \\nCan some...</td>\n      <td>0</td>\n      <td>27</td>\n      <td>2021-06-26 00:42:05</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "sdf_2.toPandas().tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(185, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "(sdf_2.count(), len(sdf_2.columns))"
   ]
  },
  {
   "source": [
    "---\n",
    "### save a sdf to a csv file with header\n",
    "\n",
    "#### The core syntax for writing data in Apache Spark\n",
    "\n",
    "DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy( ...).save()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "part-00000-da49b260-a743-4e17-a788-16cd19b03077-c000.csv  _SUCCESS\npart-00001-da49b260-a743-4e17-a788-16cd19b03077-c000.csv\n"
     ]
    }
   ],
   "source": [
    "%ls output_cof_island_sdf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(output)\n",
    "# header: str or bool, optional writes the names of columns as the first line. If None\n",
    "#  is set, it uses the default value, false.\n",
    "\n",
    "# DataFrameWriter.csv(path, mode=None, compression=None, sep=None, quote=None, escape=None, header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None, timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, charToEscapeQuoteEscaping=None, encoding=None, emptyValue=None, lineSep=None)\n",
    "# .option(\"timestampFormat\", \"MM-dd-yyyy hh mm ss\")\n",
    "# sdf_2.write.csv('output_cof_island_sdf.csv', header = 'true', mode='overwrite')\n",
    "\n",
    "# You can also use below\n",
    "sdf_2.write.format(\"csv\").mode(\"overwrite\").options(header=\"true\", escape = '\"').save(\"output_cof_island_sdf_3.csv\")"
   ]
  },
  {
   "source": [
    "---\n",
    "# Create a sdf from a csv file with header"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.read.csv(\n",
    "#     \"some_input_file.csv\", \n",
    "#     header=True, \n",
    "#     mode=\"DROPMALFORMED\", \n",
    "#     schema=schema\n",
    "# )\n",
    "\n",
    "# or\n",
    "\n",
    "# (\n",
    "#     spark.read\n",
    "#     .schema(schema)\n",
    "#     .option(\"header\", \"true\")\n",
    "#     .option(\"mode\", \"DROPMALFORMED\")\n",
    "#     .csv(\"some_input_file.csv\")\n",
    "# )\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, DateType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"favourite_count\", IntegerType()),\n",
    "    StructField(\"retweet_count\", IntegerType()),\n",
    "    StructField(\"created_at\", TimestampType())\n",
    "])\n",
    "# https://datascience.stackexchange.com/questions/12727/reading-csvs-with-new-lines-in-fields-with-spark\n",
    "sdf_2 = spark.read.csv('output_cof_island_sdf_3.csv',schema=schema,header=True,  escape = '\"',multiLine=True) #escape = '\"',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- text: string (nullable = true)\n |-- favourite_count: long (nullable = true)\n |-- retweet_count: long (nullable = true)\n |-- created_at: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "type(sdf_2)\n",
    "# pyspark.sql.dataframe.DataFrame\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "sdf_2.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['text', 'favourite_count', 'retweet_count', 'created_at']"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "sdf_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "len(sdf_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to the your code:\n",
    "\n",
    "# import pyspark\n",
    "# def spark_shape(self):\n",
    "#     return (self.count(), len(self.columns))\n",
    "# pyspark.sql.dataframe.DataFrame.shape = spark_shape\n",
    "# Then you can do\n",
    "\n",
    "# >>> df.shape()\n",
    "# (10000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 text  favourite_count  \\\n",
       "0   Lava Lei’s signature 100% Kona Coffee featurin...                1   \n",
       "1   @johnpavlovitz You might have already passed t...                0   \n",
       "2   I wonder if villagers walk round the island an...               16   \n",
       "3   @parislord @FeathersOz @thebikingvet Yes, for ...                2   \n",
       "4   @Hold2LLC @TheEliKlein I'm stuck on an island,...                0   \n",
       "..                                                ...              ...   \n",
       "95  RT @cathsherman: #Bahamas Fishing Pier, Great ...                0   \n",
       "96  @freshwaterpurl With books coffee and cinema, ...                7   \n",
       "97  RT @jesse_martin_1: One Piece, Coffee Island #...                0   \n",
       "98  RT @jesse_martin_1: One Piece, Coffee Island #...                0   \n",
       "99  RT @jesse_martin_1: One Piece, Coffee Island #...                0   \n",
       "\n",
       "    retweet_count          created_at  \n",
       "0               1 2021-06-27 23:45:08  \n",
       "1               0 2021-06-27 23:38:04  \n",
       "2               1 2021-06-27 22:35:32  \n",
       "3               0 2021-06-27 22:14:58  \n",
       "4               0 2021-06-27 22:07:46  \n",
       "..            ...                 ...  \n",
       "95             41 2021-06-27 05:02:56  \n",
       "96              0 2021-06-27 04:33:35  \n",
       "97             84 2021-06-27 04:28:01  \n",
       "98             84 2021-06-27 04:19:11  \n",
       "99             84 2021-06-27 03:52:24  \n",
       "\n",
       "[100 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>favourite_count</th>\n      <th>retweet_count</th>\n      <th>created_at</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Lava Lei’s signature 100% Kona Coffee featurin...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2021-06-27 23:45:08</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@johnpavlovitz You might have already passed t...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2021-06-27 23:38:04</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I wonder if villagers walk round the island an...</td>\n      <td>16</td>\n      <td>1</td>\n      <td>2021-06-27 22:35:32</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@parislord @FeathersOz @thebikingvet Yes, for ...</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2021-06-27 22:14:58</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@Hold2LLC @TheEliKlein I'm stuck on an island,...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2021-06-27 22:07:46</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>RT @cathsherman: #Bahamas Fishing Pier, Great ...</td>\n      <td>0</td>\n      <td>41</td>\n      <td>2021-06-27 05:02:56</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>@freshwaterpurl With books coffee and cinema, ...</td>\n      <td>7</td>\n      <td>0</td>\n      <td>2021-06-27 04:33:35</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>RT @jesse_martin_1: One Piece, Coffee Island #...</td>\n      <td>0</td>\n      <td>84</td>\n      <td>2021-06-27 04:28:01</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>RT @jesse_martin_1: One Piece, Coffee Island #...</td>\n      <td>0</td>\n      <td>84</td>\n      <td>2021-06-27 04:19:11</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>RT @jesse_martin_1: One Piece, Coffee Island #...</td>\n      <td>0</td>\n      <td>84</td>\n      <td>2021-06-27 03:52:24</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "sdf_2.sort(F.col('created_at').desc()).limit(100).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(185, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "sdf_2.toPandas().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------+-------------------+\n|                          text|         created_at|\n+------------------------------+-------------------+\n|@ThebanMonk @Ty_in_TX @El__...|2021-06-27 02:50:12|\n|***Very Strawberry &amp; Bl...|2021-06-26 14:27:32|\n+------------------------------+-------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# select columns as in a pandas dataframe\n",
    "sdf_2[['text','created_at']].show(2,truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+--------------------+\n|summary|                text|\n+-------+--------------------+\n|  count|                 185|\n|   mean|                null|\n| stddev|                null|\n|    min|***Very Strawberr...|\n|    max|🥥 Imagine a Boun...|\n+-------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "sdf_2.select('text','created_at').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+--------------------+\n|summary|                text|\n+-------+--------------------+\n|  count|                 185|\n|   mean|                null|\n| stddev|                null|\n|    min|***Very Strawberr...|\n|    max|🥥 Imagine a Boun...|\n+-------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "sdf_2[['text','created_at']].describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+---------------+-------------+----------+\n|text|favourite_count|retweet_count|created_at|\n+----+---------------+-------------+----------+\n|  @T|              2|            0|        20|\n|  **|              0|            0|        20|\n+----+---------------+-------------+----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "sdf_2[sdf_2.columns].show(2, truncate=2)"
   ]
  },
  {
   "source": [
    "---\n",
    "### def sentiment_scores & sentiment_scoresUDF\n",
    "\n",
    "ser-defined functions operate one-row-at-a-time, and thus suffer from high serialization and invocation overhead. As a result, many data pipelines define UDFs in Java and Scala and then invoke them from Python.\n",
    "\n",
    "Pandas UDFs built on top of Apache Arrow bring you the best of both worlds—the ability to define low-overhead, high-performance UDFs entirely in Python.\n",
    "\n",
    "#### Scalar Pandas UDFs\n",
    "\n",
    "Scalar Pandas UDFs are used for vectorizing scalar operations. To define a scalar Pandas UDF, simply use @pandas_udf to annotate a Python function that takes in pandas.Series as arguments and returns another pandas.Series of the same size. Below we illustrate using two examples: Plus One and Cumulative Probability.\n",
    "\n",
    "\n",
    "#### PyArrow versions\n",
    "\n",
    "PyArrow is installed in Databricks Runtime. For information on the version of PyArrow available in each Databricks Runtime version, see the Databricks runtime release notes.\n",
    "Supported SQL types\n",
    "\n",
    "All `Spark SQL data types` are supported by Arrow-based conversion except `MapType`, `ArrayType` of `TimestampType`, and nested `StructType`. `StructType` is represented as a `pandas.DataFrame` instead of `pandas.Series`. `BinaryType` is supported only when PyArrow is equal to or higher than 0.10.0.\n",
    "\n",
    "https://sparkbyexamples.com/spark/spark-sql-dataframe-data-types/\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Create DataFrame with schema"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, DateType\n",
    "\n",
    "simpleData = [[\"James \",\"\",\"Smith\",\"36636\",\"M\",3000],\n",
    "    [\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000],\n",
    "    [\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000],\n",
    "    [\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000],\n",
    "    [\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1]]\n",
    "\n",
    "simpleSchema = StructType([\n",
    "    StructField(\"firstname\",StringType()),\n",
    "    StructField(\"middlename\",StringType()),\n",
    "    StructField(\"lastname\",StringType()),\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"salary\", IntegerType())\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(simpleData, schema=simpleSchema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 255, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 88, in dump_stream\n    for batch in iterator:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 248, in init_stream_yield_batches\n    for series in iterator:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 429, in mapper\n    return f(keys, vals)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 160, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-568-e7156ae51678>\", line 60, in sentiment_scores\n  File \"/opt/anaconda/envs/pyspark_env/lib/python3.7/site-packages/nltk/sentiment/vader.py\", line 361, in polarity_scores\n    self.constants.REGEX_REMOVE_PUNCTUATION)\n  File \"/opt/anaconda/envs/pyspark_env/lib/python3.7/site-packages/nltk/sentiment/vader.py\", line 270, in __init__\n    text = str(text.encode(\"utf-8\"))\n  File \"/opt/anaconda/envs/pyspark_env/lib/python3.7/site-packages/pandas/core/generic.py\", line 5141, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'Series' object has no attribute 'encode'\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-568-e7156ae51678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# a Python native function that takes a pandas.DataFrame, and outputs a pandas.DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdf_tes_min\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyInPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rating string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 255, in dump_stream\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 88, in dump_stream\n    for batch in iterator:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 248, in init_stream_yield_batches\n    for series in iterator:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 429, in mapper\n    return f(keys, vals)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 175, in <lambda>\n    return lambda k, v: [(wrapped(k, v), to_arrow_type(return_type))]\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 160, in wrapped\n    result = f(pd.concat(value_series, axis=1))\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-568-e7156ae51678>\", line 60, in sentiment_scores\n  File \"/opt/anaconda/envs/pyspark_env/lib/python3.7/site-packages/nltk/sentiment/vader.py\", line 361, in polarity_scores\n    self.constants.REGEX_REMOVE_PUNCTUATION)\n  File \"/opt/anaconda/envs/pyspark_env/lib/python3.7/site-packages/nltk/sentiment/vader.py\", line 270, in __init__\n    text = str(text.encode(\"utf-8\"))\n  File \"/opt/anaconda/envs/pyspark_env/lib/python3.7/site-packages/pandas/core/generic.py\", line 5141, in __getattr__\n    return object.__getattribute__(self, name)\nAttributeError: 'Series' object has no attribute 'encode'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pyspark.sql.functions import udf\n",
    "import json\n",
    "# DoubleType, FloatType, ByteType, IntegerType, LongType, ShortType, ArrayType,StructField, StructType, Row\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pyspark.sql.types as Types\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# from pyspark.sql.types import DoubleType, IntegerType, StringType, DateType\n",
    "\n",
    "# simpleData = [[\"James \",\"\",\"Smith\",\"36636\",\"M\",3000],\n",
    "#     [\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000],\n",
    "#     [\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000],\n",
    "#     [\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000],\n",
    "#     [\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1]]\n",
    "\n",
    "simpleSchema = StructType([\n",
    "    StructField(\"neg\",DoubleType()),\n",
    "    StructField(\"pos\",DoubleType()),\n",
    "    StructField(\"compound\",DoubleType()),\n",
    "    StructField(\"neu\", DoubleType())\n",
    "])\n",
    "\n",
    "# df = spark.createDataFrame(simpleData, schema=simpleSchema)\n",
    "# df.printSchema()\n",
    "# df.show()\n",
    "\n",
    "# @pandas_udf('str',PandasUDFType.SCALAR)\n",
    "#  For Types.MapType() - 2 required positional arguments: 'keyType' and 'valueType'\n",
    "# @pandas_udf(simpleSchema, PandasUDFType.GROUPED_MAP)\n",
    "# @pandas_udf(Types.MapType(Types.StringType(),Types.DoubleType()))\n",
    "# @pandas_udf('struct<col1:string>')\n",
    "# def sentiment_scores(sentance: str) -> dict :\n",
    "# def sentiment_scores(pdf:pd.DataFrame) -> pd.DataFrame:\n",
    "def sentiment_scores(pdf):\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # oject gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    # ----\n",
    "    # r = sid.polarity_scores(sentance)\n",
    "\n",
    "    # pdf[r] = json.dumps(sid.polarity_scores(sentance))\n",
    "\n",
    "    # json.dumps(sid.polarity_scores(s))\n",
    "    # -----\n",
    "    # simpleSchema.neg = r.neg\n",
    "    # simpleSchema.pos = r.pos\n",
    "    # simpleSchema.compound = r.compound\n",
    "    # simpleSchema.neu = r.neu\n",
    "    # s1 = pdf['text']\n",
    "    # pdf.columns\n",
    "    # pdf['s2'] = pdf.assign(json.dumps(sid.polarity_scores(s1)))\n",
    "    # json.dumps(sid.polarity_scores(s)))\n",
    "    \n",
    "    # return r\n",
    "    # return pd.DataFrame(simpleSchema)\n",
    "    # \n",
    "    return json.dumps(sid.polarity_scores(pdf['text']))#sid.polarity_scores(s)\n",
    "    # return pdf.assign(rating = json.dumps(sid.polarity_scores(pdf)))\n",
    "    # You can optionally set the return type of your UDF. The default return type␣,→is StringType.\n",
    "    # udffactorial_p = udf(factorial_p, LongType())\n",
    "\n",
    "# sentiment_scoresUDF = udf(sentiment_scores, Types.MapType(Types.StringType(),Types.DoubleType()))\n",
    "\n",
    "# sentiment_scores_pUDF = pandas_udf(sentiment_scores, returnType=Types.MapType(Types.StringType(),Types.DoubleType()))\n",
    "\n",
    "# sentiment_scores_pUDF = pandas_udf(sentiment_scores, returnType=simpleSchema)\n",
    "\n",
    "#  udf_obj = UserDefinedFunction(\n",
    "#      42         f, returnType=returnType, name=None, evalType=evalType, deterministic=True)\n",
    "\n",
    "# sid = SentimentIntensityAnalyzer()\n",
    "# ---\n",
    "# mydata = {\"text\":['Hello wanderful world', 'Hello bad boy'], \"rank\":[0,1]}\n",
    "# pdf_mstring = pd.DataFrame(mydata)\n",
    "pdf_mstring\n",
    "# ------\n",
    "# pdf_mstring['t'] = pdf_mstring.text.apply(lambda c1:json.dumps(sid.polarity_scores(c1)))\n",
    "\n",
    "# pdf_mstring['text2'] =pdf_mstring.apply(lambda c1:sentiment_scores(c1['text']), axis=1)\n",
    "\n",
    "pdf_mstring['text2'] =pdf_mstring.apply(lambda c1:sentiment_scores(c1), axis=1)\n",
    "\n",
    "# -----\n",
    "# pdf_mstring['text2'] = pdf_mstring[['text', 'rank']].apply(lambda row: row['text'], axis=1)\n",
    "\n",
    "# pdf_mstring['text2'] = pdf_mstring[['text', 'rank']].apply(lambda row: sentiment_scores(row), axis=1)\n",
    "# pdf_mstring.assign(s2 = SentimentIntensityAnalyzer().polarity_scores(pdf_mstring.text))\n",
    "# json.dumps(sid.polarity_scores('Hello wanderful world'))\n",
    "# print(type(sid.polarity_scores('Hello wanderful world')))\n",
    "\n",
    "# sdf_test = sdf_2\n",
    "# add an index column\n",
    "# sdf_tes = sdf_test.withColumn('index', F.monotonically_increasing_id())\n",
    "# sdf_tes\n",
    "# sdf_tes_min = sdf_tes.sort('index').limit(2)\n",
    "# sdf_tes_min.show(truncate=20)\n",
    "# pyspark.sql.functions.pandas_udf(f=None, returnType=None, functionType=None)\n",
    "# a Python native function that takes a pandas.DataFrame, and outputs a pandas.DataFrame.\n",
    "ss = sdf_tes_min.select('text', 'index').groupby(\"text\").applyInPandas(sentiment_scores, schema=\"rating string\")\n",
    "ss.show()\n",
    "# \n",
    "\n",
    "# \n",
    "# pdf_mstring.drop('text2', axis=1, inplace = True)\n",
    "# pdf_mstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+---------------+-------------+-------------------+-----+\n|                text|favourite_count|retweet_count|         created_at|index|\n+--------------------+---------------+-------------+-------------------+-----+\n|There's a lot hap...|              0|            0|2021-06-25 16:43:44|    0|\n|There's a lot hap...|              0|            0|2021-06-25 16:43:44|    1|\n+--------------------+---------------+-------------+-------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "sdf_tes_min.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+----------+-----------------+--------------------+\n|long_col|string_col|       struct_col|                 new|\n+--------+----------+-----------------+--------------------+\n|       1|  a string|[a nested string]|[a nested string, 9]|\n+--------+----------+-----------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [[1, \"a string\", (\"a nested string\",)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
    "\n",
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def pandas_plus_len(\n",
    "        s1: pd.Series, s2: pd.Series, pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Regular columns are series and the struct column is a DataFrame.\n",
    "    pdf['col2'] = s1 + s2.str.len() \n",
    "    return pdf  # the struct column expects a DataFrame to return\n",
    "\n",
    "df.withColumn('new',pandas_plus_len(\"long_col\", \"string_col\", \"struct_col\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---+---+\n",
      "| id|pdf|sum|\n",
      "+---+---+---+\n",
      "|  0|  0|  0|\n",
      "|  1|  2|  3|\n",
      "|  2|  4|  6|\n",
      "|  3|  6|  9|\n",
      "|  4|  8| 12|\n",
      "|  5| 10| 15|\n",
      "|  6| 12| 18|\n",
      "|  7| 14| 21|\n",
      "|  8| 16| 24|\n",
      "|  9| 18| 27|\n",
      "+---+---+---+\n",
      "\n",
      "+--------+\n",
      "|max(pdf)|\n",
      "+--------+\n",
      "|      18|\n",
      "+--------+\n",
      "\n",
      "Row(max(pdf)=18)\n",
      "18\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n/opt/spark/python/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\\n#   \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 358
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType,col\n",
    "\n",
    "\n",
    "# @pandas_udf('double', PandasUDFType.SCALAR)\n",
    "# def pandas_plus_one(v):\n",
    "#     # `v` is a pandas Series\n",
    "#     return v.add(1)  # outputs a pandas Series\n",
    "\n",
    "@pandas_udf('string') #return value\n",
    "def pandas_sum_two_columns(s1: pd.Series, s2: pd.Series) -> pd.Series:\n",
    "    # return s.add(1)\n",
    "    # pdf2 = pdf + s\n",
    "    # return (s1 + s2).astype(str)\n",
    "    return (s1 + s2).astype(str)\n",
    "\n",
    "\n",
    "\n",
    "sdf_pandas_udf_ex1 = spark.range(10).withColumn('pdf',col(\"id\")*2).withColumn('sum', pandas_sum_two_columns(\"id\", \"pdf\"))\n",
    "sdf_pandas_udf_ex1.show()\n",
    "sdf_pandas_udf_ex1.agg({'pdf':'max'}).show()\n",
    "print(sdf_pandas_udf_ex1.agg({'pdf':'max'}).collect()[0])\n",
    "print(sdf_pandas_udf_ex1.agg({'pdf':'max'}).collect()[0][0])\n",
    "\n",
    "'''\n",
    "/opt/spark/python/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
    "#   \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0    1\n",
      "1    4\n",
      "2    9\n",
      "dtype: int64\n",
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  4|\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a * b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())\n",
    "\n",
    "# The function for a pandas_udf should be able to execute with local pandas data\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))\n",
    "# 0    1\n",
    "# 1    4\n",
    "# 2    9\n",
    "# dtype: int64\n",
    "\n",
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(multiply(col(\"x\"), col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+---+\n| id|pdf|\n+---+---+\n|  0|  0|\n|  1|  2|\n|  2|  4|\n|  3|  6|\n|  4|  8|\n|  5| 10|\n|  6| 12|\n|  7| 14|\n|  8| 16|\n|  9| 18|\n+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "# spark.range(10).withColumn('pdf',col(\"id\").cast('string')).show()\n",
    "spark.range(10).withColumn('pdf',col(\"id\")*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  1| 2.0|\n",
      "|  2| 3.0|\n",
      "|  2| 5.0|\n",
      "|  2|10.0|\n",
      "+---+----+\n",
      "\n",
      "+---+----+-------------------+\n",
      "| id|   v|         normalized|\n",
      "+---+----+-------------------+\n",
      "|  1| 1.0|-0.7071067811865475|\n",
      "|  1| 2.0| 0.7071067811865475|\n",
      "|  2| 3.0|-0.8320502943378437|\n",
      "|  2| 5.0|-0.2773500981126146|\n",
      "|  2|10.0| 1.1094003924504583|\n",
      "+---+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "from pyspark.sql.functions import pandas_udf, ceil\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\"))  \n",
    "def normalize(pdf):\n",
    "    v = pdf.v\n",
    "    # Assign new columns to a DataFrame\n",
    "    return pdf.assign(normalized=(v - v.mean()) / v.std())\n",
    "df.show()\n",
    "df.groupby(\"id\").applyInPandas(\n",
    "    normalize, schema=\"id long, v double, normalized double\").show() "
   ]
  },
  {
   "source": [
    "---\n",
    "### def sentiment_scores & sentiment_scoresUDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.functions import udf\n",
    "# DoubleType, FloatType, ByteType, IntegerType, LongType, ShortType, ArrayType,StructField, StructType, Row\n",
    "# from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pyspark.sql.types as Types\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def sentiment_scores(sentance: str) -> dict :\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # oject gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    r = sid.polarity_scores(sentance)\n",
    "    return r\n",
    "    # You can optionally set the return type of your UDF. The default return type␣,→is StringType.\n",
    "    # udffactorial_p = udf(factorial_p, LongType())\n",
    "\n",
    "sentiment_scoresUDF = udf(sentiment_scores, Types.MapType(Types.StringType(),Types.DoubleType()))"
   ]
  },
  {
   "source": [
    "---\n",
    "### sdf\n",
    "\n",
    "create a new column with sentiment_scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,sqrt,log,reverse\n",
    "sdf_2 = sdf_2.withColumn(\"rating\", sentiment_scoresUDF(sdf_2.text))\n",
    "\n",
    "# df.groupby(\"id\").applyInPandas(\n",
    "#     normalize, schema=\"id long, v double, normalized double\").show()\n",
    "# sdf_udf = sdf_2.groupby(\"text\").applyInPandas(\n",
    "    # sentiment_scores, schema=\"text string, rating string\")\n",
    "\n",
    "# sdf_pudf.show(2,truncate=20)\n",
    "# t.show()\n",
    "# sdf_2.toPandas().style.set_properties(subset=['text'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------+---------------+-------------+-------------------+------------------------------+\n|                          text|favourite_count|retweet_count|         created_at|                        rating|\n+------------------------------+---------------+-------------+-------------------+------------------------------+\n|@ThebanMonk @Ty_in_TX @El__...|              2|            0|2021-06-27 02:50:12|[neg -> 0.0, pos -> 0.0, co...|\n|***Very Strawberry &amp; Bl...|              0|            0|2021-06-26 14:27:32|[neg -> 0.032, pos -> 0.06,...|\n+------------------------------+---------------+-------------+-------------------+------------------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "sdf_2.show(2, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:88: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n  Unsupported type in conversion to Arrow: MapType(StringType,DoubleType,true)\nAttempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n  warnings.warn(msg)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdecc987590>"
      ],
      "text/html": "<style  type=\"text/css\" >\n#T_4d572e5c_d851_11eb_9bf1_0800278e5397row0_col0,#T_4d572e5c_d851_11eb_9bf1_0800278e5397row1_col0{\n            width:  300px;\n        }</style><table id=\"T_4d572e5c_d851_11eb_9bf1_0800278e5397\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >text</th>        <th class=\"col_heading level0 col1\" >favourite_count</th>        <th class=\"col_heading level0 col2\" >retweet_count</th>        <th class=\"col_heading level0 col3\" >created_at</th>        <th class=\"col_heading level0 col4\" >rating</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_4d572e5c_d851_11eb_9bf1_0800278e5397level0_row0\" class=\"row_heading level0 row0\" >0</th>\n                        <td id=\"T_4d572e5c_d851_11eb_9bf1_0800278e5397row0_col0\" class=\"data row0 col0\" >@ThebanMonk @Ty_in_TX @El__Bohemio @historybythpint @VeroniqueSemtex @TheRogue_Elf @heartsabustin @AllanKirkhart @i_ourpatio @vetjr89 @AltWoodstone @WorldhopperVive @Ajah1551 @GhostieMingo @beautatas @dieseldave97 @GentlemanRascal @meggymish @IAMISjp @blc3428 @BillHaggis @LadyDemosthenes @jr_bohl @freehorse8 @melly_stone @mddebm @John_Roberts__ @john_iamme @AmericanPurrl @KieranEleison @cgogolin @emmdub559 @rathernotsay @RantyAmyCurtis @Philly_Hoosier @Prissi_coffee @AWGecko @EarthSalter @MeerkatYitz And only one island instead of the whole world.</td>\n                        <td id=\"T_4d572e5c_d851_11eb_9bf1_0800278e5397row0_col1\" class=\"data row0 col1\" >2</td>\n                        <td id=\"T_4d572e5c_d851_11eb_9bf1_0800278e5397row0_col2\" class=\"data row0 col2\" >0</td>\n                        <td id=\"T_4d572e5c_d851_11eb_9bf1_0800278e5397row0_col3\" class=\"data row0 col3\" >2021-06-27 02:50:12</td>\n                        <td id=\"T_4d572e5c_d851_11eb_9bf1_0800278e5397row0_col4\" class=\"data row0 col4\" >{'neg': 0.0, 'pos': 0.0, 'compound': 0.0, 'neu': 1.0}</td>\n            </tr>\n            <tr>\n                        <th id=\"T_4d572e5c_d851_11eb_9bf1_0800278e5397level0_row1\" class=\"row_heading level0 row1\" >1</th>\n                        <td id=\"T_4d572e5c_d851_11eb_9bf1_0800278e5397row1_col0\" class=\"data row1 col0\" >***Very Strawberry &amp; Blue Cotton Candy***\nChocolate Ice Cream &amp; Vanilla Ice Cream\nNSA Vanilla &amp; Campfire Crush\nNew York Cheesecake &amp; Pineapple Dole Soft Serve\nCold Brew Coffee Gelato &amp; Cake Batter\nSweet Coconut &amp; Island Banana\nCookie n' Cream &amp; Caramel Sea Salt Gelato https://t.co/qT474mhoyy</td>\n                        <td id=\"T_4d572e5c_d851_11eb_9bf1_0800278e5397row1_col1\" class=\"data row1 col1\" >0</td>\n                        <td id=\"T_4d572e5c_d851_11eb_9bf1_0800278e5397row1_col2\" class=\"data row1 col2\" >0</td>\n                        <td id=\"T_4d572e5c_d851_11eb_9bf1_0800278e5397row1_col3\" class=\"data row1 col3\" >2021-06-26 14:27:32</td>\n                        <td id=\"T_4d572e5c_d851_11eb_9bf1_0800278e5397row1_col4\" class=\"data row1 col4\" >{'neg': 0.032, 'pos': 0.06, 'compound': 0.34, 'neu': 0.907}</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sqrt,log,reverse\n",
    "sdf_2 = sdf_2.withColumn(\"rating\", sentiment_scoresUDF(sdf_2.text))\n",
    "# t.show()\n",
    "sdf_2.limit(2).toPandas().style.set_properties(subset=['text'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2a56ca5c10>"
      ],
      "text/html": "<style  type=\"text/css\" >\n#T_4dd40856_d459_11eb_b3b3_0800278e5397row0_col0,#T_4dd40856_d459_11eb_b3b3_0800278e5397row1_col0{\n            width:  300px;\n        }</style><table id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >text</th>        <th class=\"col_heading level0 col1\" >favourite_count</th>        <th class=\"col_heading level0 col2\" >retweet_count</th>        <th class=\"col_heading level0 col3\" >created_at</th>        <th class=\"col_heading level0 col4\" >negative_nltk</th>        <th class=\"col_heading level0 col5\" >positive_nltk</th>        <th class=\"col_heading level0 col6\" >neutral_nltk</th>        <th class=\"col_heading level0 col7\" >compound_nltk</th>        <th class=\"col_heading level0 col8\" >rating</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397level0_row0\" class=\"row_heading level0 row0\" >0</th>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row0_col0\" class=\"data row0 col0\" >@breakfasttv I received the Astrazeneca vaccine on April 9th. No, I do not regret it! I look forward to getting the Pfizer vaccine on the 28th of this month. I feel confident we will be fully protected. Even if it is unconventional. 😊</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row0_col1\" class=\"data row0 col1\" >0</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row0_col2\" class=\"data row0 col2\" >0</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row0_col3\" class=\"data row0 col3\" >2021-06-20</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row0_col4\" class=\"data row0 col4\" >0.049000</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row0_col5\" class=\"data row0 col5\" >0.199000</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row0_col6\" class=\"data row0 col6\" >0.752000</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row0_col7\" class=\"data row0 col7\" >0.779300</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row0_col8\" class=\"data row0 col8\" >{'neg': 0.049, 'pos': 0.199, 'compound': 0.7793, 'neu': 0.752}</td>\n            </tr>\n            <tr>\n                        <th id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397level0_row1\" class=\"row_heading level0 row1\" >1</th>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row1_col0\" class=\"data row1 col0\" >#Pfizer\n#AstraZeneca\n#Moderna\n#JohnsonAndJohnson \n\nWhat's up !  Safe and effective !\n\n⬇️\n⬇️ https://t.co/fvw1H8cmXT</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row1_col1\" class=\"data row1 col1\" >3</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row1_col2\" class=\"data row1 col2\" >5</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row1_col3\" class=\"data row1 col3\" >2021-06-20</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row1_col4\" class=\"data row1 col4\" >0.000000</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row1_col5\" class=\"data row1 col5\" >0.397000</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row1_col6\" class=\"data row1 col6\" >0.603000</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row1_col7\" class=\"data row1 col7\" >0.763900</td>\n                        <td id=\"T_4dd40856_d459_11eb_b3b3_0800278e5397row1_col8\" class=\"data row1 col8\" >{'neg': 0.0, 'pos': 0.397, 'compound': 0.7639, 'neu': 0.603}</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "sdf.toPandas().head(2).style.set_properties(subset=['text'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2a4876bb90>"
      ],
      "text/html": "<style  type=\"text/css\" >\n#T_309f510e_d45a_11eb_b3b3_0800278e5397row0_col0,#T_309f510e_d45a_11eb_b3b3_0800278e5397row1_col0{\n            width:  300px;\n        }</style><table id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >text</th>        <th class=\"col_heading level0 col1\" >favourite_count</th>        <th class=\"col_heading level0 col2\" >retweet_count</th>        <th class=\"col_heading level0 col3\" >created_at</th>        <th class=\"col_heading level0 col4\" >negative_nltk</th>        <th class=\"col_heading level0 col5\" >positive_nltk</th>        <th class=\"col_heading level0 col6\" >neutral_nltk</th>        <th class=\"col_heading level0 col7\" >compound_nltk</th>        <th class=\"col_heading level0 col8\" >rating</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397level0_row0\" class=\"row_heading level0 row0\" >6508</th>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row0_col0\" class=\"data row0 col0\" >RT @JohnRHewson: For the record, how many of our political leaders had Pfizer rather than AstraZeneca?</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row0_col1\" class=\"data row0 col1\" >0</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row0_col2\" class=\"data row0 col2\" >890</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row0_col3\" class=\"data row0 col3\" >2021-06-19</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row0_col4\" class=\"data row0 col4\" >0.000000</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row0_col5\" class=\"data row0 col5\" >0.000000</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row0_col6\" class=\"data row0 col6\" >1.000000</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row0_col7\" class=\"data row0 col7\" >0.000000</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row0_col8\" class=\"data row0 col8\" >{'neg': 0.0, 'pos': 0.0, 'compound': 0.0, 'neu': 1.0}</td>\n            </tr>\n            <tr>\n                        <th id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397level0_row1\" class=\"row_heading level0 row1\" >6509</th>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row1_col0\" class=\"data row1 col0\" >RT @DrEricDing: 3) “After years of reading research on mixing vaccine types -- known as heterologous prime-boosting -- Morgon concluded tha…</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row1_col1\" class=\"data row1 col1\" >0</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row1_col2\" class=\"data row1 col2\" >55</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row1_col3\" class=\"data row1 col3\" >2021-06-19</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row1_col4\" class=\"data row1 col4\" >0.000000</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row1_col5\" class=\"data row1 col5\" >0.000000</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row1_col6\" class=\"data row1 col6\" >1.000000</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row1_col7\" class=\"data row1 col7\" >0.000000</td>\n                        <td id=\"T_309f510e_d45a_11eb_b3b3_0800278e5397row1_col8\" class=\"data row1 col8\" >{'neg': 0.0, 'pos': 0.0, 'compound': 0.0, 'neu': 1.0}</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "sdf.toPandas().tail(2).style.set_properties(subset=['text'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------+---------------+-------------+----------+-------------+-------------+------------+-------------+------------------------------+\n|                          text|favourite_count|retweet_count|created_at|negative_nltk|positive_nltk|neutral_nltk|compound_nltk|                        rating|\n+------------------------------+---------------+-------------+----------+-------------+-------------+------------+-------------+------------------------------+\n|@breakfasttv I received the...|              0|            0|2021-06-20|        0.049|        0.199|       0.752|       0.7793|[neg -> 0.049, pos -> 0.199...|\n|#Pfizer\n#AstraZeneca\n#Moder...|              3|            5|2021-06-20|          0.0|        0.397|       0.603|       0.7639|[neg -> 0.0, pos -> 0.397, ...|\n+------------------------------+---------------+-------------+----------+-------------+-------------+------------+-------------+------------------------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "sdf.show(2, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(text='@breakfasttv I received the Astrazeneca vaccine on April 9th. No, I do not regret it! I look forward to getting the Pfizer vaccine on the 28th of this month. I feel confident we will be fully protected. Even if it is unconventional. 😊', favourite_count=0, retweet_count=0, created_at=datetime.date(2021, 6, 20), rating={'neg': 0.049, 'pos': 0.199, 'compound': 0.7793, 'neu': 0.752}),\n",
       " Row(text=\"#Pfizer\\n#AstraZeneca\\n#Moderna\\n#JohnsonAndJohnson \\n\\nWhat's up !  Safe and effective !\\n\\n⬇️\\n⬇️ https://t.co/fvw1H8cmXT\", favourite_count=3, retweet_count=5, created_at=datetime.date(2021, 6, 20), rating={'neg': 0.0, 'pos': 0.397, 'compound': 0.7639, 'neu': 0.603})]"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "# Returns the first num rows as a list of Row.\n",
    "# This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.\n",
    "sdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(text='RT @JohnRHewson: For the record, how many of our political leaders had Pfizer rather than AstraZeneca?', favourite_count=0, retweet_count=890, created_at=datetime.date(2021, 6, 19), rating={'neg': 0.0, 'pos': 0.0, 'compound': 0.0, 'neu': 1.0}),\n",
       " Row(text='RT @DrEricDing: 3) “After years of reading research on mixing vaccine types -- known as heterologous prime-boosting -- Morgon concluded tha…', favourite_count=0, retweet_count=55, created_at=datetime.date(2021, 6, 19), rating={'neg': 0.0, 'pos': 0.0, 'compound': 0.0, 'neu': 1.0})]"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "# Returns the last num rows as a list of Row.\n",
    "sdf.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- text: string (nullable = true)\n |-- favourite_count: integer (nullable = true)\n |-- retweet_count: integer (nullable = true)\n |-- created_at: date (nullable = true)\n |-- negative_nltk: double (nullable = true)\n |-- positive_nltk: double (nullable = true)\n |-- neutral_nltk: double (nullable = true)\n |-- compound_nltk: double (nullable = true)\n |-- rating: map (nullable = true)\n |    |-- key: string\n |    |-- value: double (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    " from pyspark.sql.functions import col \n",
    "\n",
    " sdf_2 = sdf_2.withColumn('negative_nltk', col('rating')['neg']) \\\n",
    ".withColumn('positive_nltk', col('rating')['pos']) \\\n",
    ".withColumn('neutral_nltk', col('rating')['neu']) \\\n",
    ".withColumn('compound_nltk',col('rating')['compound']) \\\n",
    ".drop('rating')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdebe325650>"
      ],
      "text/html": "<style  type=\"text/css\" >\n#T_518ba812_d852_11eb_9bf1_0800278e5397row0_col0,#T_518ba812_d852_11eb_9bf1_0800278e5397row1_col0{\n            width:  300px;\n        }</style><table id=\"T_518ba812_d852_11eb_9bf1_0800278e5397\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >text</th>        <th class=\"col_heading level0 col1\" >favourite_count</th>        <th class=\"col_heading level0 col2\" >retweet_count</th>        <th class=\"col_heading level0 col3\" >created_at</th>        <th class=\"col_heading level0 col4\" >negative_nltk</th>        <th class=\"col_heading level0 col5\" >positive_nltk</th>        <th class=\"col_heading level0 col6\" >neutral_nltk</th>        <th class=\"col_heading level0 col7\" >compound_nltk</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_518ba812_d852_11eb_9bf1_0800278e5397level0_row0\" class=\"row_heading level0 row0\" >0</th>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row0_col0\" class=\"data row0 col0\" >@ThebanMonk @Ty_in_TX @El__Bohemio @historybythpint @VeroniqueSemtex @TheRogue_Elf @heartsabustin @AllanKirkhart @i_ourpatio @vetjr89 @AltWoodstone @WorldhopperVive @Ajah1551 @GhostieMingo @beautatas @dieseldave97 @GentlemanRascal @meggymish @IAMISjp @blc3428 @BillHaggis @LadyDemosthenes @jr_bohl @freehorse8 @melly_stone @mddebm @John_Roberts__ @john_iamme @AmericanPurrl @KieranEleison @cgogolin @emmdub559 @rathernotsay @RantyAmyCurtis @Philly_Hoosier @Prissi_coffee @AWGecko @EarthSalter @MeerkatYitz And only one island instead of the whole world.</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row0_col1\" class=\"data row0 col1\" >2</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row0_col2\" class=\"data row0 col2\" >0</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row0_col3\" class=\"data row0 col3\" >2021-06-27 02:50:12</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row0_col4\" class=\"data row0 col4\" >0.000000</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row0_col5\" class=\"data row0 col5\" >0.000000</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row0_col6\" class=\"data row0 col6\" >1.000000</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row0_col7\" class=\"data row0 col7\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_518ba812_d852_11eb_9bf1_0800278e5397level0_row1\" class=\"row_heading level0 row1\" >1</th>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row1_col0\" class=\"data row1 col0\" >***Very Strawberry &amp; Blue Cotton Candy***\nChocolate Ice Cream &amp; Vanilla Ice Cream\nNSA Vanilla &amp; Campfire Crush\nNew York Cheesecake &amp; Pineapple Dole Soft Serve\nCold Brew Coffee Gelato &amp; Cake Batter\nSweet Coconut &amp; Island Banana\nCookie n' Cream &amp; Caramel Sea Salt Gelato https://t.co/qT474mhoyy</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row1_col1\" class=\"data row1 col1\" >0</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row1_col2\" class=\"data row1 col2\" >0</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row1_col3\" class=\"data row1 col3\" >2021-06-26 14:27:32</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row1_col4\" class=\"data row1 col4\" >0.032000</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row1_col5\" class=\"data row1 col5\" >0.060000</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row1_col6\" class=\"data row1 col6\" >0.907000</td>\n                        <td id=\"T_518ba812_d852_11eb_9bf1_0800278e5397row1_col7\" class=\"data row1 col7\" >0.340000</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "sdf_2.limit(2).toPandas().style.set_properties(subset=['text'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f2a4d305750>"
      ],
      "text/html": "<style  type=\"text/css\" >\n#T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row0_col0,#T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row1_col0{\n            width:  300px;\n        }</style><table id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >text</th>        <th class=\"col_heading level0 col1\" >favourite_count</th>        <th class=\"col_heading level0 col2\" >retweet_count</th>        <th class=\"col_heading level0 col3\" >created_at</th>        <th class=\"col_heading level0 col4\" >negative_nltk</th>        <th class=\"col_heading level0 col5\" >positive_nltk</th>        <th class=\"col_heading level0 col6\" >neutral_nltk</th>        <th class=\"col_heading level0 col7\" >compound_nltk</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397level0_row0\" class=\"row_heading level0 row0\" >6508</th>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row0_col0\" class=\"data row0 col0\" >RT @JohnRHewson: For the record, how many of our political leaders had Pfizer rather than AstraZeneca?</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row0_col1\" class=\"data row0 col1\" >0</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row0_col2\" class=\"data row0 col2\" >890</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row0_col3\" class=\"data row0 col3\" >2021-06-19</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row0_col4\" class=\"data row0 col4\" >0.000000</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row0_col5\" class=\"data row0 col5\" >0.000000</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row0_col6\" class=\"data row0 col6\" >1.000000</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row0_col7\" class=\"data row0 col7\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397level0_row1\" class=\"row_heading level0 row1\" >6509</th>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row1_col0\" class=\"data row1 col0\" >RT @DrEricDing: 3) “After years of reading research on mixing vaccine types -- known as heterologous prime-boosting -- Morgon concluded tha…</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row1_col1\" class=\"data row1 col1\" >0</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row1_col2\" class=\"data row1 col2\" >55</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row1_col3\" class=\"data row1 col3\" >2021-06-19</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row1_col4\" class=\"data row1 col4\" >0.000000</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row1_col5\" class=\"data row1 col5\" >0.000000</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row1_col6\" class=\"data row1 col6\" >1.000000</td>\n                        <td id=\"T_ebd5efbc_d45c_11eb_b3b3_0800278e5397row1_col7\" class=\"data row1 col7\" >0.000000</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "sdf.toPandas().tail(2).style.set_properties(subset=['text'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------+---------------+-------------+---------------+-------------+-------------+------------+-------------+\n|           text|favourite_count|retweet_count|     created_at|negative_nltk|positive_nltk|neutral_nltk|compound_nltk|\n+---------------+---------------+-------------+---------------+-------------+-------------+------------+-------------+\n|Good Morning...|              0|            0|2021-06-27 1...|          0.0|        0.266|       0.734|       0.4404|\n|Good Morning...|              0|            0|2021-06-26 1...|          0.0|        0.266|       0.734|       0.4404|\n+---------------+---------------+-------------+---------------+-------------+-------------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "sdf_from_list_of_rows = spark.createDataFrame(sdf_2.tail(2))\n",
    "\n",
    "sdf_from_list_of_rows.show(truncate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+---------------+-------------+----------+-------------+-------------+------------+-------------+\n|                text|favourite_count|retweet_count|created_at|negative_nltk|positive_nltk|neutral_nltk|compound_nltk|\n+--------------------+---------------+-------------+----------+-------------+-------------+------------+-------------+\n|RT @JohnRHewson: ...|              0|          890|2021-06-19|          0.0|          0.0|         1.0|          0.0|\n|RT @DrEricDing: 3...|              0|           55|2021-06-19|          0.0|          0.0|         1.0|          0.0|\n+--------------------+---------------+-------------+----------+-------------+-------------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "sdf_from_pdf = spark.createDataFrame(pdf)\n",
    "\n",
    "sdf_from_list_of_rows.show(2)"
   ]
  },
  {
   "source": [
    "https://stackoverflow.com/questions/61608057/output-vader-sentiment-scores-in-columns-based-on-dataframe-rows-of-tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Create the dataframe\n",
    "# df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\", 3)], [\"letter\", \"name\"])\n",
    "\n",
    "# Function to get rows at `rownums`\n",
    "# def getrows(df, rownums=None):\n",
    "    # return df.rdd.zipWithIndex().filter(lambda x: x[1] in rownums).map(lambda x: x[0])\n",
    "\n",
    "# Get rows at positions 0 and 2.\n",
    "# getrows(df, rownums=[0, 2]).collect()"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "source": [
    "# Spark GroupBy and Aggregate Functions\n",
    "\n",
    "`GroupBy` allows you to group rows together based off some column value, for example, you could group together sales data by the day the sale occured, or group repeast customer data based off the name of the customer.\n",
    "\n",
    "Once you've performed the GroupBy operation you can use an aggregate function off that data.An `aggregate function` aggregates multiple rows of data into a single output, such as taking the sum of inputs, or counting the number of inputs.\n",
    "\n",
    "**`Dataframe Aggregation`**\n",
    "\n",
    "A set of methods for aggregations on a DataFrame:\n",
    "\n",
    "    agg\n",
    "    avg\n",
    "    count\n",
    "    max\n",
    "    mean\n",
    "    min\n",
    "    pivot\n",
    "    sum\n",
    "\n",
    "https://hendra-herviawan.github.io/pyspark-groupby-and-aggregate-functions.html\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['text',\n",
       " 'favourite_count',\n",
       " 'retweet_count',\n",
       " 'created_at',\n",
       " 'negative_nltk',\n",
       " 'positive_nltk',\n",
       " 'neutral_nltk',\n",
       " 'compound_nltk']"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "sdf_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[created_at: timestamp]"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "sdf_2.select('created_at')"
   ]
  },
  {
   "source": [
    "```\n",
    "sdf_agg_byDate =  sdf.groupBy('created_at').agg({'negative_nltk':'sum'}, {'positive_nltk':'sum'}, {'neutral_nltk':'sum'},\t{'compound_nltk':'sum'})\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sdf_agg_byDate =  sdf.groupBy('created_at').agg({'negative_nltk':'sum','positive_nltk':'sum','neutral_nltk':'sum','compound_nltk':'sum','created_at':'count'})\n",
    "\n",
    "sdf_agg_byDate.count()\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf.groupBy('created_at').count().select('created_at', f.col('count').alias('tweets')).show()"
   ]
  },
  {
   "source": [
    "### Convert Timestamp to Date\n",
    "\n",
    "> Syntax: to_date(timestamp_column)\n",
    "\n",
    "> Syntax: to_date(timestamp_column,format)\n",
    "\n",
    "PySpark timestamp (TimestampType) consists of value in the format yyyy-MM-dd HH:mm:ss.SSSS and Date (DateType) format would be yyyy-MM-dd. Use to_date() function to truncate time from Timestamp or to convert the timestamp to date on DataFrame column."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+---------------+-------------+----------+-------------+-------------+------------+-------------+\n|                text|favourite_count|retweet_count|created_at|negative_nltk|positive_nltk|neutral_nltk|compound_nltk|\n+--------------------+---------------+-------------+----------+-------------+-------------+------------+-------------+\n|@ThebanMonk @Ty_i...|              2|            0|2021-06-27|          0.0|          0.0|         1.0|          0.0|\n|***Very Strawberr...|              0|            0|2021-06-26|        0.032|         0.06|       0.907|         0.34|\n+--------------------+---------------+-------------+----------+-------------+-------------+------------+-------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "sdf_2 = sdf_2.withColumn('created_at', to_date(F.col('created_at')))\n",
    "sdf_2.show(2, truncate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fdeccb3be10>"
      ],
      "text/html": "<style  type=\"text/css\" >\n#T_8314ed3a_d857_11eb_9bf1_0800278e5397row0_col0{\n            width:  300px;\n        }</style><table id=\"T_8314ed3a_d857_11eb_9bf1_0800278e5397\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >text</th>        <th class=\"col_heading level0 col1\" >favourite_count</th>        <th class=\"col_heading level0 col2\" >retweet_count</th>        <th class=\"col_heading level0 col3\" >created_at</th>        <th class=\"col_heading level0 col4\" >negative_nltk</th>        <th class=\"col_heading level0 col5\" >positive_nltk</th>        <th class=\"col_heading level0 col6\" >neutral_nltk</th>        <th class=\"col_heading level0 col7\" >compound_nltk</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_8314ed3a_d857_11eb_9bf1_0800278e5397level0_row0\" class=\"row_heading level0 row0\" >0</th>\n                        <td id=\"T_8314ed3a_d857_11eb_9bf1_0800278e5397row0_col0\" class=\"data row0 col0\" >@ThebanMonk @Ty_in_TX @El__Bohemio @historybythpint @VeroniqueSemtex @TheRogue_Elf @heartsabustin @AllanKirkhart @i_ourpatio @vetjr89 @AltWoodstone @WorldhopperVive @Ajah1551 @GhostieMingo @beautatas @dieseldave97 @GentlemanRascal @meggymish @IAMISjp @blc3428 @BillHaggis @LadyDemosthenes @jr_bohl @freehorse8 @melly_stone @mddebm @John_Roberts__ @john_iamme @AmericanPurrl @KieranEleison @cgogolin @emmdub559 @rathernotsay @RantyAmyCurtis @Philly_Hoosier @Prissi_coffee @AWGecko @EarthSalter @MeerkatYitz And only one island instead of the whole world.</td>\n                        <td id=\"T_8314ed3a_d857_11eb_9bf1_0800278e5397row0_col1\" class=\"data row0 col1\" >2</td>\n                        <td id=\"T_8314ed3a_d857_11eb_9bf1_0800278e5397row0_col2\" class=\"data row0 col2\" >0</td>\n                        <td id=\"T_8314ed3a_d857_11eb_9bf1_0800278e5397row0_col3\" class=\"data row0 col3\" >2021-06-27</td>\n                        <td id=\"T_8314ed3a_d857_11eb_9bf1_0800278e5397row0_col4\" class=\"data row0 col4\" >0.000000</td>\n                        <td id=\"T_8314ed3a_d857_11eb_9bf1_0800278e5397row0_col5\" class=\"data row0 col5\" >0.000000</td>\n                        <td id=\"T_8314ed3a_d857_11eb_9bf1_0800278e5397row0_col6\" class=\"data row0 col6\" >1.000000</td>\n                        <td id=\"T_8314ed3a_d857_11eb_9bf1_0800278e5397row0_col7\" class=\"data row0 col7\" >0.000000</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "sdf_2.toPandas().head(1).style.set_properties(subset=['text'], **{'width': '300px'})"
   ]
  },
  {
   "source": [
    "```\n",
    "from pyspark.sql.functions import to_date\n",
    "sdf_aggg = (sdf_2\n",
    ".groupBy(to_date(F.col('created_at')).alias('created_at'))\n",
    ".agg(F.count('created_at').alias('tweets'))\n",
    ".show())\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+-------------------+------------------+-----------------+------------------+------------------+\n|created_at| sum(compound_nltk)|sum(positive_nltk)|count(created_at)|sum(negative_nltk)| sum(neutral_nltk)|\n+----------+-------------------+------------------+-----------------+------------------+------------------+\n|2021-06-20| -36.28830000000009| 112.9250000000001|             2407|           133.614|2160.4989999999916|\n|2021-06-19|-106.83589999999874| 139.2199999999993|             4103|176.49200000000073|3787.2440000000083|\n+----------+-------------------+------------------+-----------------+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "sdf_agg_byDate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf_agg_byDate = sdf_agg_byDate.withColumn(\"sum\", col(\"sum(negative_nltk)\")+col(\"science_score\"))\n",
    "# \tdf1.show()"
   ]
  },
  {
   "source": [
    "### groupBy and aggregate on multiple columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'created_at': 'count',\n",
       " 'negative_nltk': 'sum',\n",
       " 'positive_nltk': 'sum',\n",
       " 'neutral_nltk': 'sum',\n",
       " 'compound_nltk': 'sum'}"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "exprs={}\n",
    "cols = ['created_at',\n",
    " 'negative_nltk',\n",
    " 'positive_nltk',\n",
    " 'neutral_nltk',\n",
    " 'compound_nltk']\n",
    "exprs = {x: \"sum\" for x in cols}\n",
    "exprs['created_at'] = 'count'\n",
    "exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "source": [
    "# sdf_agg_byDate = sdf.groupBy('created_at').agg({'negative_nltk':'sum'}, {'positive_nltk':'sum'}, {'neutral_nltk':'sum'}, {'compound_nltk':'sum'})\n",
    "import pyspark.sql.functions as f \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sdf_agg_byDate = sdf_2.groupBy('created_at').agg({'negative_nltk':'sum','positive_nltk':'sum','neutral_nltk':'sum','compound_nltk':'sum','created_at':'count'}).withColumnRenamed('count(created_at)', 'tweets')\n",
    "\n",
    "sdf_agg_byDate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   created_at  sum(compound_nltk)  sum(positive_nltk)  tweets  \\\n",
       "0  2021-06-27             16.0636               7.043     115   \n",
       "\n",
       "   sum(negative_nltk)  sum(neutral_nltk)  \n",
       "0               1.753            106.202  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>created_at</th>\n      <th>sum(compound_nltk)</th>\n      <th>sum(positive_nltk)</th>\n      <th>tweets</th>\n      <th>sum(negative_nltk)</th>\n      <th>sum(neutral_nltk)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-06-27</td>\n      <td>16.0636</td>\n      <td>7.043</td>\n      <td>115</td>\n      <td>1.753</td>\n      <td>106.202</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "sdf_agg_byDate.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf.groupBy(\"department\",\"state\") \\\n",
    "#     .sum(\"salary\",\"bonus\") \\\n",
    "#     .show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   created_at  sum(compound_nltk)  sum(positive_nltk)  tweets  \\\n",
       "0  2021-06-27             16.0636               7.043     115   \n",
       "\n",
       "   sum(negative_nltk)  sum(neutral_nltk)  \n",
       "0               1.753            106.202  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>created_at</th>\n      <th>sum(compound_nltk)</th>\n      <th>sum(positive_nltk)</th>\n      <th>tweets</th>\n      <th>sum(negative_nltk)</th>\n      <th>sum(neutral_nltk)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-06-27</td>\n      <td>16.0636</td>\n      <td>7.043</td>\n      <td>115</td>\n      <td>1.753</td>\n      <td>106.202</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "sdf_2.groupBy('created_at') \\\n",
    "    .agg(exprs).withColumnRenamed('count(created_at)', 'tweets') \\\n",
    "    .limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   created_at  tweets  compound_nltk  positive_nltk  negativen_ltk  \\\n",
       "0  2021-06-27     115       0.139683       0.061243       0.015243   \n",
       "\n",
       "   neutral_nltk  \n",
       "0      0.923496  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>created_at</th>\n      <th>tweets</th>\n      <th>compound_nltk</th>\n      <th>positive_nltk</th>\n      <th>negativen_ltk</th>\n      <th>neutral_nltk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-06-27</td>\n      <td>115</td>\n      <td>0.139683</td>\n      <td>0.061243</td>\n      <td>0.015243</td>\n      <td>0.923496</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "# How to delete columns in pyspark dataframe\n",
    "columns_to_drop = ['sum(compound_nltk)', 'sum(positive_nltk)', 'sum(negative_nltk)', 'sum(neutral_nltk)']\n",
    "\n",
    "sdf_agg_byDate = (sdf_agg_byDate\n",
    "    .withColumn('compound_nltk', f.col('sum(compound_nltk)')/F.col('tweets'))\n",
    "    .withColumn( 'positive_nltk',f.col('sum(positive_nltk)')/F.col('tweets'))\n",
    "    .withColumn( 'negativen_ltk',f.col('sum(negative_nltk)')/F.col('tweets'))\n",
    "    .withColumn( 'neutral_nltk',f.col('sum(neutral_nltk)')/F.col('tweets'))\n",
    "    .drop(*columns_to_drop)\n",
    "    )\n",
    "\n",
    "sdf_agg_byDate.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   created_at  tweets  compound_nltk  positive_nltk  negativen_ltk  \\\n",
       "0  2021-06-27     115       0.139683       0.061243       0.015243   \n",
       "1  2021-06-26      70       0.198057       0.102986       0.032043   \n",
       "\n",
       "   neutral_nltk  \n",
       "0      0.923496  \n",
       "1      0.864957  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>created_at</th>\n      <th>tweets</th>\n      <th>compound_nltk</th>\n      <th>positive_nltk</th>\n      <th>negativen_ltk</th>\n      <th>neutral_nltk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-06-27</td>\n      <td>115</td>\n      <td>0.139683</td>\n      <td>0.061243</td>\n      <td>0.015243</td>\n      <td>0.923496</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-06-26</td>\n      <td>70</td>\n      <td>0.198057</td>\n      <td>0.102986</td>\n      <td>0.032043</td>\n      <td>0.864957</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 164
    }
   ],
   "source": [
    "sdf_agg_byDate.toPandas()\n",
    "# .drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Column<b'created_at[0]'>"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "source": [
    "sdf.created_at[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below statement changes column 'count(created_at)' to 'tweets' on PySpark DataFrame. \n",
    "# sdf_agg_byDate = (sdf_agg_byDate\n",
    "#     .withColumnRenamed('count(created_at)', 'tweets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   created_at  tweets  compound_nltk  positive_nltk  negativen_ltk  \\\n",
       "0  2021-06-26      70       0.198057       0.102986       0.032043   \n",
       "1  2021-06-27     115       0.139683       0.061243       0.015243   \n",
       "\n",
       "   neutral_nltk  \n",
       "0      0.864957  \n",
       "1      0.923496  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>created_at</th>\n      <th>tweets</th>\n      <th>compound_nltk</th>\n      <th>positive_nltk</th>\n      <th>negativen_ltk</th>\n      <th>neutral_nltk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-06-26</td>\n      <td>70</td>\n      <td>0.198057</td>\n      <td>0.102986</td>\n      <td>0.032043</td>\n      <td>0.864957</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-06-27</td>\n      <td>115</td>\n      <td>0.139683</td>\n      <td>0.061243</td>\n      <td>0.015243</td>\n      <td>0.923496</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "source": [
    "# DataFrame.sort(*cols, **kwargs) - Returns a new DataFrame sorted by the specified column(s).\n",
    "sdf_agg_byDate = sdf_agg_byDate.sort('created_at')\n",
    "\n",
    "sdf_agg_byDate.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('created_at', 'date'),\n",
       " ('tweets', 'bigint'),\n",
       " ('compound_nltk', 'double'),\n",
       " ('positive_nltk', 'double'),\n",
       " ('negativen_ltk', 'double'),\n",
       " ('neutral_nltk', 'double')]"
      ]
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "source": [
    "# \n",
    "sdf_agg_byDate.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(created_at=datetime.date(2021, 6, 19), tweets=4103, compound_nltk=-0.026038484036070862, positive_nltk=0.033931269802583305, negativen_ltk=0.043015354618571956, neutral_nltk=0.923042651718257)]"
      ]
     },
     "metadata": {},
     "execution_count": 225
    }
   ],
   "source": [
    "sdf_agg_byDate.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------+--------------------+--------------------+--------------------+-----------------+\n|created_at|tweets|       compound_nltk|       positive_nltk|       negativen_ltk|     neutral_nltk|\n+----------+------+--------------------+--------------------+--------------------+-----------------+\n|2021-06-19|  4103|-0.02603848403607...|0.033931269802583305|0.043015354618571956|0.923042651718257|\n+----------+------+--------------------+--------------------+--------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "sdf_agg_byDate.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Row(created_at=datetime.date(2021, 6, 19), tweets=4103, compound_nltk=-0.026038484036070862, positive_nltk=0.033931269802583305, negativen_ltk=0.043015354618571956, neutral_nltk=0.923042651718257)"
      ]
     },
     "metadata": {},
     "execution_count": 223
    }
   ],
   "source": [
    "# last, head, tail\n",
    "sdf_agg_byDate.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(datetime.date(2021, 6, 26), datetime.date(2021, 6, 28))"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    "since, until"
   ]
  },
  {
   "source": [
    "### to_date() – Convert String to Date Format\n",
    "\n",
    "Syntax: to_date(column,format)\n",
    "\n",
    "Example: to_date(col(\"string_column\"),\"MM-dd-yyyy\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Column<b\"to_date('2021-06-26')\">, Column<b\"to_date('2021-06-27')\">)"
      ]
     },
     "metadata": {},
     "execution_count": 168
    }
   ],
   "source": [
    "# from pyspark.sql.functions import lit\n",
    "# dates = (\"2021-06-26\",  \"2021-06-27\")\n",
    "# date_from, date_to = [to_date(lit(s)).cast(TimestampType()) for s in dates]\n",
    "# (date_from, date_to)\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "dates = (\"2021-06-26\",  \"2021-06-27\")\n",
    "date_from, date_to = [to_date(lit(s)) for s in dates]\n",
    "(date_from, date_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------+-------------------+-------------------+--------------------+-----------------+\n|created_at|tweets|      compound_nltk|      positive_nltk|       negativen_ltk|     neutral_nltk|\n+----------+------+-------------------+-------------------+--------------------+-----------------+\n|2021-06-27|   115|0.13968347826086958|0.06124347826086956|0.015243478260869567|0.923495652173913|\n+----------+------+-------------------+-------------------+--------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# import datetime, time \n",
    "# dates = (\"2021-06-26 00:00:00\",  \"2021-06-27 00:00:00\")\n",
    "# # date_from, date_to = (\"2021-06-26\",  \"2021-06-27\")\n",
    "\n",
    "# timestamps = (\n",
    "#     time.mktime(datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S\").timetuple())\n",
    "#     for s in dates)\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "\n",
    "sdf_agg_byDate.filter(col('created_at') > date_from).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+------+-------------------+-------------------+--------------------+------------------+\n|created_at|tweets|      compound_nltk|      positive_nltk|       negativen_ltk|      neutral_nltk|\n+----------+------+-------------------+-------------------+--------------------+------------------+\n|2021-06-26|    70|0.19805714285714288|0.10298571428571429|0.032042857142857144|0.8649571428571429|\n|2021-06-27|   115|0.13968347826086958|0.06124347826086956|0.015243478260869567| 0.923495652173913|\n+----------+------+-------------------+-------------------+--------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "sdf_agg_byDate.filter(col('created_at') >= date_from).show()"
   ]
  },
  {
   "source": [
    "`sf = sf.filter(sf.my_col >= date_from).filter(sf.my_col <= date_to)\n",
    "sf.count()`\n",
    "\n",
    "https://stackoverflow.com/questions/31407461/datetime-range-filter-in-pyspark-sql\n",
    "\n",
    "\n",
    "https://sparkbyexamples.com/pyspark/pyspark-difference-between-two-dates-days-months-years/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "sdf_agg_byDate.filter(sdf_agg_byDate['created_at'] >= date_from).filter(sdf_agg_byDate['created_at'] <= date_to).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 176
    }
   ],
   "source": [
    "sdf_agg_byDate.select('*', sdf_agg_byDate['created_at'].between(date_from, date_to)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   created_at  tweets  compound_nltk  positive_nltk  negativen_ltk  \\\n",
       "0  2021-06-26      70       0.198057       0.102986       0.032043   \n",
       "1  2021-06-27     115       0.139683       0.061243       0.015243   \n",
       "\n",
       "   neutral_nltk  \\\n",
       "0      0.864957   \n",
       "1      0.923496   \n",
       "\n",
       "   ((created_at >= to_date('2021-06-26')) AND (created_at <= to_date('2021-06-27')))  \n",
       "0                                               True                                  \n",
       "1                                               True                                  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>created_at</th>\n      <th>tweets</th>\n      <th>compound_nltk</th>\n      <th>positive_nltk</th>\n      <th>negativen_ltk</th>\n      <th>neutral_nltk</th>\n      <th>((created_at &gt;= to_date('2021-06-26')) AND (created_at &lt;= to_date('2021-06-27')))</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-06-26</td>\n      <td>70</td>\n      <td>0.198057</td>\n      <td>0.102986</td>\n      <td>0.032043</td>\n      <td>0.864957</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-06-27</td>\n      <td>115</td>\n      <td>0.139683</td>\n      <td>0.061243</td>\n      <td>0.015243</td>\n      <td>0.923496</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 178
    }
   ],
   "source": [
    "sdf_agg_byDate.select('*', sdf_agg_byDate['created_at'].between(date_from, date_to)).toPandas()"
   ]
  },
  {
   "source": [
    "### Evaluate sentiment: \n",
    "\n",
    "- positive -> 1, \n",
    "- negative -> -1, \n",
    "- neutral -> 0"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.functions import udf\n",
    "# DoubleType, FloatType, ByteType, IntegerType, LongType, ShortType, ArrayType,StructField, StructType, Row\n",
    "# from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pyspark.sql.types as Types\n",
    "\n",
    "\n",
    "def sentiment_eval(comp_score: float) -> int :\n",
    "\n",
    "    # if compound_score > 0.05 => 1 i.e positive\n",
    "    if comp_score > 0.05:\n",
    "        return 1\n",
    "    elif comp_score < 0.05:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "sentiment_evalUDF = udf(sentiment_eval, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_agg_byDate['sentiment'] = (sdf_agg_byDate['compound_nltk']\n",
    "        .apply(lambda comp: 'positive' if comp > 0.05 else 'negative' if comp < -0.05 else 'neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "elapsed: 13.501638650894165\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sdf_agg_byDate.withColumn('sentiment', sentiment_evalUDF(col('compound_nltk'))).toPandas()\n",
    "end=time.time()\n",
    "print(f'elapsed: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf('float')\n",
    "def sentiment_eval_pUDF(comp_score: pd.Series) -> pd.Series:\n",
    "    s=[]\n",
    "    # if compound_score > 0.05 => 1 i.e positive\n",
    "    for elmnt in comp_score:\n",
    "        if elmnt > 0.05:\n",
    "            s.append(1)\n",
    "        elif elmnt < 0.05:\n",
    "            s.append(-1)\n",
    "        else:\n",
    "            s.append(0)\n",
    "    return pd.Series(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\nFalse\nTrue\n********************\nTrue\nFalse\n"
     ]
    }
   ],
   "source": [
    "x = pd.Series([0, 1, 2])\n",
    "for el in x:\n",
    "    print((el > 1))\n",
    "print('*'*20)\n",
    "print(x.any())   # because one element is zero\n",
    "print(x.all())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "elapsed: 12.960050106048584\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sdf_agg_byDate.withColumn('sentiment', sentiment_eval_pUDF(col('compound_nltk'))).toPandas()\n",
    "end=time.time()\n",
    "print(f'elapsed: {end-start}')\n",
    "\n",
    "# File \"<ipython-input-248-44cbe12222e7>\", line 7, in sentiment_eval_pUDF\n",
    "#   File \"/opt/anaconda/envs/pyspark_env/lib/python3.7/site-packages/pandas/core/generic.py\", line 1330, in __nonzero__\n",
    "#     f\"The truth value of a {type(self).__name__} is ambiguous. \"\n",
    "# ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
    "\n",
    "# https://stackoverflow.com/questions/36921951/truth-value-of-a-series-is-ambiguous-use-a-empty-a-bool-a-item-a-any-o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_agg_byDate = sdf_agg_byDate.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   created_at  tweets  compound_nltk  positive_nltk  negativen_ltk  \\\n",
       "0  2021-06-26      70       0.198057       0.102986       0.032043   \n",
       "1  2021-06-27     115       0.139683       0.061243       0.015243   \n",
       "\n",
       "   neutral_nltk  \n",
       "0      0.864957  \n",
       "1      0.923496  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>created_at</th>\n      <th>tweets</th>\n      <th>compound_nltk</th>\n      <th>positive_nltk</th>\n      <th>negativen_ltk</th>\n      <th>neutral_nltk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-06-26</td>\n      <td>70</td>\n      <td>0.198057</td>\n      <td>0.102986</td>\n      <td>0.032043</td>\n      <td>0.864957</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-06-27</td>\n      <td>115</td>\n      <td>0.139683</td>\n      <td>0.061243</td>\n      <td>0.015243</td>\n      <td>0.923496</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 275
    }
   ],
   "source": [
    "pdf_agg_byDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "elapsed: 0.030269145965576172\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "pdf_agg_byDate['sentiment'] = (pdf_agg_byDate['compound_nltk']\n",
    "        .apply(lambda comp: 'positive' if comp > 0.05 else 'negative' if comp < -0.05 else 'neutral'))\n",
    "end=time.time()\n",
    "print(f'elapsed: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   created_at  tweets  compound_nltk  positive_nltk  negativen_ltk  \\\n",
       "0  2021-06-26      70       0.198057       0.102986       0.032043   \n",
       "1  2021-06-27     115       0.139683       0.061243       0.015243   \n",
       "\n",
       "   neutral_nltk sentiment  \n",
       "0      0.864957  positive  \n",
       "1      0.923496  positive  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>created_at</th>\n      <th>tweets</th>\n      <th>compound_nltk</th>\n      <th>positive_nltk</th>\n      <th>negativen_ltk</th>\n      <th>neutral_nltk</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-06-26</td>\n      <td>70</td>\n      <td>0.198057</td>\n      <td>0.102986</td>\n      <td>0.032043</td>\n      <td>0.864957</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-06-27</td>\n      <td>115</td>\n      <td>0.139683</td>\n      <td>0.061243</td>\n      <td>0.015243</td>\n      <td>0.923496</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 278
    }
   ],
   "source": [
    "pdf_agg_byDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n",
      " |-- long_col: long (nullable = true)\n",
      " |-- string_col: string (nullable = true)\n",
      " |-- struct_col: struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      "\n",
      "+--------+----------+-----------------+\n",
      "|long_col|string_col|       struct_col|\n",
      "+--------+----------+-----------------+\n",
      "|       1|  a string|[a nested string]|\n",
      "+--------+----------+-----------------+\n",
      "\n",
      "+--------+----------+-----------------+--------------------+\n",
      "|long_col|string_col|       struct_col|             derived|\n",
      "+--------+----------+-----------------+--------------------+\n",
      "|       1|  a string|[a nested string]|[a nested string, 9]|\n",
      "+--------+----------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3\n",
    "# Create a Spark DataFrame that has three columns including a sturct column.\n",
    "df = spark.createDataFrame(\n",
    "[[1, \"a string\", (\"a nested string\",)]],\n",
    "\"long_col long, string_col string, struct_col struct<col1:string>\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "\n",
    "df = df.withColumn('derived', func(\"long_col\", \"string_col\", \"struct_col\"))\n",
    "# Create a Spark DataFrame that has three columns including a sturct column.\n",
    "\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "source": [
    "### Best practices for successfully managing memory for Apache Spark applications on Amazon EMR\n",
    "\n",
    "https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Set sparkContext and sqlContext"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "# sqlContext.sql(\"get spark.sql.shuffle.partitions=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spark.eventLog.enabled : true\nspark.app.id : local-1624903764057\nspark.driver.host : spark-client\nspark.driver.port : 36309\nspark.repl.local.jars : file:///home/hadoopuser/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-3.0.0.jar,file:///home/hadoopuser/.ivy2/jars/com.typesafe_config-1.3.0.jar,file:///home/hadoopuser/.ivy2/jars/org.rocksdb_rocksdbjni-6.5.3.jar,file:///home/hadoopuser/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.603.jar,file:///home/hadoopuser/.ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,file:///home/hadoopuser/.ivy2/jars/com.navigamez_greex-1.0.jar,file:///home/hadoopuser/.ivy2/jars/org.json4s_json4s-ext_2.12-3.5.3.jar,file:///home/hadoopuser/.ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.2.2.jar,file:///home/hadoopuser/.ivy2/jars/net.sf.trove4j_trove4j-3.0.3.jar,file:///home/hadoopuser/.ivy2/jars/com.google.code.findbugs_annotations-3.0.1.jar,file:///home/hadoopuser/.ivy2/jars/com.google.protobuf_protobuf-java-util-3.0.0-beta-3.jar,file:///home/hadoopuser/.ivy2/jars/com.google.protobuf_protobuf-java-3.0.0-beta-3.jar,file:///home/hadoopuser/.ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar,file:///home/hadoopuser/.ivy2/jars/org.projectlombok_lombok-1.16.8.jar,file:///home/hadoopuser/.ivy2/jars/org.slf4j_slf4j-api-1.7.21.jar,file:///home/hadoopuser/.ivy2/jars/net.jcip_jcip-annotations-1.0.jar,file:///home/hadoopuser/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.1.jar,file:///home/hadoopuser/.ivy2/jars/com.google.code.gson_gson-2.3.jar,file:///home/hadoopuser/.ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar,file:///home/hadoopuser/.ivy2/jars/joda-time_joda-time-2.9.5.jar,file:///home/hadoopuser/.ivy2/jars/org.joda_joda-convert-1.8.1.jar\nspark.executor.id : driver\nspark.eventLog.dir : file:///opt/spark/logs/spark-events/spark-logs\nspark.history.fs.logDirectory : file:///opt/spark/logs/spark-events/spark-logs\nspark.submit.pyFiles : /home/hadoopuser/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-3.0.0.jar,/home/hadoopuser/.ivy2/jars/com.typesafe_config-1.3.0.jar,/home/hadoopuser/.ivy2/jars/org.rocksdb_rocksdbjni-6.5.3.jar,/home/hadoopuser/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.603.jar,/home/hadoopuser/.ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,/home/hadoopuser/.ivy2/jars/com.navigamez_greex-1.0.jar,/home/hadoopuser/.ivy2/jars/org.json4s_json4s-ext_2.12-3.5.3.jar,/home/hadoopuser/.ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.2.2.jar,/home/hadoopuser/.ivy2/jars/net.sf.trove4j_trove4j-3.0.3.jar,/home/hadoopuser/.ivy2/jars/com.google.code.findbugs_annotations-3.0.1.jar,/home/hadoopuser/.ivy2/jars/com.google.protobuf_protobuf-java-util-3.0.0-beta-3.jar,/home/hadoopuser/.ivy2/jars/com.google.protobuf_protobuf-java-3.0.0-beta-3.jar,/home/hadoopuser/.ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar,/home/hadoopuser/.ivy2/jars/org.projectlombok_lombok-1.16.8.jar,/home/hadoopuser/.ivy2/jars/org.slf4j_slf4j-api-1.7.21.jar,/home/hadoopuser/.ivy2/jars/net.jcip_jcip-annotations-1.0.jar,/home/hadoopuser/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.1.jar,/home/hadoopuser/.ivy2/jars/com.google.code.gson_gson-2.3.jar,/home/hadoopuser/.ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar,/home/hadoopuser/.ivy2/jars/joda-time_joda-time-2.9.5.jar,/home/hadoopuser/.ivy2/jars/org.joda_joda-convert-1.8.1.jar\nspark.app.name : Spark_and_Pandas_twitter_dfs\nspark.rdd.compress : True\nspark.serializer.objectStreamReset : 100\nspark.master : local[*]\nspark.jars : file:///home/hadoopuser/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-3.0.0.jar,file:///home/hadoopuser/.ivy2/jars/com.typesafe_config-1.3.0.jar,file:///home/hadoopuser/.ivy2/jars/org.rocksdb_rocksdbjni-6.5.3.jar,file:///home/hadoopuser/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.603.jar,file:///home/hadoopuser/.ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,file:///home/hadoopuser/.ivy2/jars/com.navigamez_greex-1.0.jar,file:///home/hadoopuser/.ivy2/jars/org.json4s_json4s-ext_2.12-3.5.3.jar,file:///home/hadoopuser/.ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.2.2.jar,file:///home/hadoopuser/.ivy2/jars/net.sf.trove4j_trove4j-3.0.3.jar,file:///home/hadoopuser/.ivy2/jars/com.google.code.findbugs_annotations-3.0.1.jar,file:///home/hadoopuser/.ivy2/jars/com.google.protobuf_protobuf-java-util-3.0.0-beta-3.jar,file:///home/hadoopuser/.ivy2/jars/com.google.protobuf_protobuf-java-3.0.0-beta-3.jar,file:///home/hadoopuser/.ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar,file:///home/hadoopuser/.ivy2/jars/org.projectlombok_lombok-1.16.8.jar,file:///home/hadoopuser/.ivy2/jars/org.slf4j_slf4j-api-1.7.21.jar,file:///home/hadoopuser/.ivy2/jars/net.jcip_jcip-annotations-1.0.jar,file:///home/hadoopuser/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.1.jar,file:///home/hadoopuser/.ivy2/jars/com.google.code.gson_gson-2.3.jar,file:///home/hadoopuser/.ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar,file:///home/hadoopuser/.ivy2/jars/joda-time_joda-time-2.9.5.jar,file:///home/hadoopuser/.ivy2/jars/org.joda_joda-convert-1.8.1.jar\nspark.submit.deployMode : client\nspark.files : file:///home/hadoopuser/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-3.0.0.jar,file:///home/hadoopuser/.ivy2/jars/com.typesafe_config-1.3.0.jar,file:///home/hadoopuser/.ivy2/jars/org.rocksdb_rocksdbjni-6.5.3.jar,file:///home/hadoopuser/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.603.jar,file:///home/hadoopuser/.ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,file:///home/hadoopuser/.ivy2/jars/com.navigamez_greex-1.0.jar,file:///home/hadoopuser/.ivy2/jars/org.json4s_json4s-ext_2.12-3.5.3.jar,file:///home/hadoopuser/.ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.2.2.jar,file:///home/hadoopuser/.ivy2/jars/net.sf.trove4j_trove4j-3.0.3.jar,file:///home/hadoopuser/.ivy2/jars/com.google.code.findbugs_annotations-3.0.1.jar,file:///home/hadoopuser/.ivy2/jars/com.google.protobuf_protobuf-java-util-3.0.0-beta-3.jar,file:///home/hadoopuser/.ivy2/jars/com.google.protobuf_protobuf-java-3.0.0-beta-3.jar,file:///home/hadoopuser/.ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar,file:///home/hadoopuser/.ivy2/jars/org.projectlombok_lombok-1.16.8.jar,file:///home/hadoopuser/.ivy2/jars/org.slf4j_slf4j-api-1.7.21.jar,file:///home/hadoopuser/.ivy2/jars/net.jcip_jcip-annotations-1.0.jar,file:///home/hadoopuser/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.1.jar,file:///home/hadoopuser/.ivy2/jars/com.google.code.gson_gson-2.3.jar,file:///home/hadoopuser/.ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar,file:///home/hadoopuser/.ivy2/jars/joda-time_joda-time-2.9.5.jar,file:///home/hadoopuser/.ivy2/jars/org.joda_joda-convert-1.8.1.jar\nspark.ui.showConsoleProgress : true\n"
     ]
    }
   ],
   "source": [
    "# sqlContext.sql(\"set spark.sql.shuffle.partitions=10\")\n",
    "# spark.default.parallelism is the default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set explicitly by the user. Note that spark.default.parallelism seems to only be working for raw RDD and is ignored when working with dataframes.\n",
    "\n",
    "# If the task you are performing is not a join or aggregation and you are working with dataframes then setting these will not have any effect. You could, however, set the number of partitions yourself by calling df.repartition(numOfPartitions) (don't forget to assign it to a new val) in your code.\n",
    "# sqlContext.sql(\"get spark.sql.shuffle.partitions\")\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "# spark.conf.get(\"spark.default.parallelism\")\n",
    "# sc.getConf().getAll()\n",
    "# sc.getConf(\"spark.default.parallelism\")\n",
    "# sc.conf.get(\"spark.driver.memory\")\n",
    "# spark.sparkContext.get(spark.sql.shuffle.partitions)#spark.sql.functions.partitions\n",
    "\n",
    "configurations = spark.sparkContext.getConf().getAll()\n",
    "for conf in configurations:\n",
    "    print(conf[0],':',conf[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "spark.sql.shuffle.partitions = 200\n"
     ]
    }
   ],
   "source": [
    "configurations = spark.sparkContext.getConf().getAll()\n",
    "for conf in configurations:\n",
    "    if conf[0] == 'spark.sql.shuffle.partitions':\n",
    "        print(conf[0],':',conf[1])\n",
    "\n",
    "print(f'spark.sql.shuffle.partitions = {spark.conf.get(\"spark.sql.shuffle.partitions\")}')\n",
    "# spark.sql.shuffle.partitions = 200\n",
    "# spark.executor.id "
   ]
  },
  {
   "source": [
    "# perfplot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "888ad0ae7731483fb3b8c5d2faa93965"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import perfplot\n",
    "\n",
    "perfplot.save(\n",
    "    \"out.png\",\n",
    "    setup=lambda n: pd.DataFrame(np.arange(n * 3).reshape(n, 3)),\n",
    "    n_range=[2**k for k in range(25)],\n",
    "    kernels=[\n",
    "        lambda df: len(df.index),\n",
    "        lambda df: df.shape[0],\n",
    "        lambda df: df[df.columns[0]].count(),\n",
    "    ],\n",
    "    labels=[\"len(df.index)\", \"df.shape[0]\", \"df[df.columns[0]].count()\"],\n",
    "    xlabel=\"Number of rows\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "c027416a0caaaf5e4fd3b25f66102efea033e33be1f6b9d7ac7164ef671d094d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}